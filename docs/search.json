[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Gao",
    "section": "",
    "text": "Welcome to my website!\nI’m a Business Analyst! !"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nBrian Gao\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nBrian Gao\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n456 Supply Chain HW 1\n\n\n\n\n\n\nBrian Gao\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Brian Gao’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project1/hw1_questions.html",
    "href": "projects/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#introduction",
    "href": "projects/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#data",
    "href": "projects/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\ndata = pd.read_stata('data/karlan_list_2007.dta')\n\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nNumber of months since last donation (mrm2)\n\n# Extracting the data for treatment and control groups\ntreatment_group = data[data['treatment'] == 1]['mrm2']\ncontrol_group = data[data['control'] == 1]['mrm2']\n\n# Dropping NA values for the t-test\ntreatment_group = treatment_group.dropna()\ncontrol_group = control_group.dropna()\n\n# Calculating the components of the t-statistic manually\nmean_treatment = treatment_group.mean()\nmean_control = control_group.mean()\nstd_treatment = treatment_group.std()\nstd_control = control_group.std()\nn_treatment = treatment_group.count()\nn_control = control_group.count()\n\n# Calculating the t-statistic using the formula\nnumerator = mean_treatment - mean_control\ndenominator = np.sqrt((std_treatment**2 / n_treatment) + (std_control**2 / n_control))\nt_statistic_manual = numerator / denominator\n\n# Performing the t-test using scipy's built-in t-test function for independent samples\nt_statistic_auto, p_value_auto = stats.ttest_ind(treatment_group, control_group, equal_var=False)\n\n# Comparing results from manual calculation and automatic function\n(t_statistic_manual, t_statistic_auto, p_value_auto)\n\nreg = rsm.model.regress(\n    data={\"data\": data},\n    rvar=\"mrm2\",\n    evar=\"treatment\"\n)\nreg.summary()\n\nLinear regression (OLS)\nData                 : data\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082\n\n\nBy using t-test, I got the t-value for months since last donation approximately equal to 0.1195 and the corresponding p-value approximately equal to 0.9049 which we agree with the null hypothesis that the mean values for the treatment and control are equal for the mrm2 variable.\nThe coefficient for treatment is 0.014, suggesting that being in the treatment group is associated with an average increase in mrm2 of 0.014 units compared to the control group. However, this effect is not statistically significant (p = 0.905), implying that the treatment does not have a meaningful impact on mrm2. The t-value of 0.119 is far below any conventional critical value for significance (around ±1.96 for a 95% confidence level), reinforcing this finding. Lastly, the result is exactly the same compare to t-test.\nNumber of years since initial donation (years)\nBy using similar codes, I got the t-value for Number of years since initial donation approximately equal to -1.0909 and the corresponding p-value approximately equal to 0.2753 which we agree with the null hypothesis that the mean values for the treatment and control are equal for the years variable.\nThe coefficient for treatment is 0.009, suggesting that being in the treatment group is associated with an average increase in years of 0.009 units compared to the control group. However, this effect is not statistically significant (p = 0.27), implying that the treatment does not have a meaningful impact on years. The t-value of -1.103 is far from any conventional critical value for significance (around ±1.96 for a 95% confidence level), reinforcing this finding. Lastly, the result is slightly off compare to t-test.\nNumber of prior donations (freq)\nBy using similar codes, I got the t-value for Number of prior donations approximately equal to -0.1108 and the corresponding p-value approximately equal to 0.9117 which we agree with the null hypothesis that the mean values for the treatment and control are equal for the freq variable.\nThe coefficient for treatment is -0.012, suggesting that being in the treatment group is associated with an average decrease in freq of 0.012 units compared to the control group. However, this effect is not statistically significant (p = 0.912), implying that the treatment does not have a meaningful impact on freq. The t-value of -1.111 is far from any conventional critical value for significance (around ±1.96 for a 95% confidence level), reinforcing this finding. Lastly, the result is the same compare to t-test."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#experimental-results",
    "href": "projects/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\n\n\nt_statistic_manual: 3.2094621908279835 p-value: 0.0013309823450914173\nLinear regression (OLS)\nData                 : data\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe statistical tests performed — both the t-test and the bivariate linear regression — suggest that there is a statistically significant difference between the treatment and control groups regarding making a donation. The small p-values in both tests (t-test and regression) tell us that the observed differences in donation rates between the control and treatment groups are highly unlikely to have occurred by chance (p &lt; 0.05). This provides strong evidence in favor of the treatment’s impact on increasing donation rates. The results indicate that even small incentives or changes in how donations are solicited (like matching donations) can positively affect donor behavior. This aligns with behavioral economic theories suggesting that people are more likely to engage in pro-social behavior (like donating to charity) if they perceive their contribution as being more impactful or if they receive some form of positive reinforcement or matching contribution.\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nTue, 30 Apr 2024\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n16:57:07\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\nThese results match column 1 in table 3, confirming the positive and significant impact of treatment on the likelihood of making a charitable donation.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n1:1 vs 2:1 t_statistic: -0.965048975142932  p-value: 0.33453078237183076\n1:1 vs 3:1 t_statistic: -1.0150174470156275  p-value: 0.31010856527625774\n2:1 vs 3:1 t_statistic: -0.05011581369764474  p-value: 0.9600305476940865\n\n\nMy results support the “figures suggest” comment since my results also show that the ratios does not have a meaningful influence on behavior(p-value much higher than 0.05).\n\n\nLinear regression (OLS)\nData                 : data\nResponse variable    : gave\nExplanatory variables: ratio\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio[1]         0.003      0.002    1.661   0.097    .\nratio[2]         0.005      0.002    2.744   0.006   **\nratio[3]         0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nThe regression analysis assessing the impact of different match ratios on the likelihood of making a donation reveals that higher match ratios (2:1 and 3:1) significantly increase the probability of donating by approximately 0.5 percentage points compared to the baseline, a statistically significant finding with p-values of 0.006 and 0.005, respectively. Although the 1:1 match ratio also shows an increase in donation probability (by 0.3 percentage points), this effect is only marginally significant (p-value of 0.097).\n\n\ndirect_1:1 vs 2:1: 0.0019 direct_2:1 vs 3:1: 0.0001\ncoef_1:1 vs 2:1: 0.002 coef_2:1 vs 3:1: 0.0\n\n\nThe analysis reveals that increasing the match ratio from 1:1 to 2:1 enhances the probability of donation by approximately 0.188% (direct calculation) and predicts a 0.2% increase based on regression coefficients, indicating a statistically significant effect that suggests a higher match ratio effectively encourages more donations. However, further increasing the match ratio from 2:1 to 3:1 shows a negligible increase in donation likelihood (only 0.01% as per direct calculation) and no effect according to regression coefficients (0.0%), suggesting that beyond a 2:1 match, there are diminishing returns in terms of motivating additional donations. This pattern underscores the effectiveness of moderate increases in match ratios while highlighting a threshold beyond which higher ratios do not yield proportional gains in donor engagement.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nThe t-test indicates a relatively weak statistically significant difference in donation amounts between the treatment and control groups (p-value=0.055). By performing these analyses, we learn whether the treatment effectively increases donation amounts and quantifies the increase. This helps in understanding the financial impact of the treatment and can guide future decisions regarding the use of such treatments to boost donations.\nWhat happens if we limit the data to just people who made a donation and repeat the previous analysis?\n\n\nLinear regression (OLS)\nData                 : donors_data\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nOn average, individuals in the control group (or baseline category, assuming treatment = 0) donate $45.54. This is significantly different from zero (p &lt; .001), suggesting a high confidence in this average donation amount among the control group donors.\nBeing in the treatment group is associated with a decrease in the donation amount by $1.668 compared to the control group, although this effect is not statistically significant (p = 0.561). This implies that there is no strong evidence to suggest that treatment status influences the donation amount among those who chose to donate.\nIf the treatment was randomly assigned among participants, then the coefficient can be interpreted causally as the effect of the treatment on donation amounts among donors. This causal interpretation is valid under the assumption of random assignment."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#simulation-experiment",
    "href": "projects/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\nThese histograms will visually demonstrate the convergence property described by the Central Limit Theorem: as the sample size increases, the distribution of the sample mean (of differences, in this case) will approximate a normal distribution more closely, centering around the true mean difference (0.004) with decreasing variance. If zero is frequently not in the middle as sample sizes increase, it supports the hypothesis that the treatment does indeed have a real, positive effect on the likelihood of donations.\n\nimport pandas as pd\ndata = {\n    'Names': ['Alan', 'Bob', 'Charles'],\n    'Ages': [22, 33, 44]\n}\n\n# Create the DataFrame with an index\ndf = pd.DataFrame(data, index=['Data1', 'Data2', 'Data3'])\n\nprint(df)\n\n         Names  Ages\nData1     Alan    22\nData2      Bob    33\nData3  Charles    44"
  },
  {
    "objectID": "projects/project2/456_HW1.html",
    "href": "projects/project2/456_HW1.html",
    "title": "456 Supply Chain HW 1",
    "section": "",
    "text": "Consider that the publisher, Scribner, acquired your bookstore, BN, i.e., you manage the whole supply chain, both the Publisher and the bookstore. In this case, what is the cost of understocking, the cost of overstocking, and the target service level of the supply chain (Note that the supply chain here means the Publisher and the bookstore)? How many books the supply chain should print? In this case, what is the supply chain average profit based on the data?\n\ncost of understocking(cu)\n\nThe lost profit for each book not sold when there is demand is essentially the retail price minus the wholesale price (since if we had the book, we could have sold it at retail price and replenished at wholesale price).\nCu=Retail Price−Wholesale Price=$22−$2=$20 per book\n\ncost of overstocking(co)\n\nThe cost of each unsold book is the cost of printing because once you’ve purchased the book, if not sold, the loss is what was paid to print it (considering you manage both the printing/publishing and the selling as one entity now).\nCo=Cost to Publisher to print =$2 per book\n\nTarget Service Level (Service Factor)\n\nThe target service level determines the probability that there will be no stockouts, which depends on how the costs of overstocking and understocking relate. The critical ratio (CR) is used to determine the target service level and is defined as: \\[(\\frac{Cu}{Cu+Co})\\]\nhere, Cu=$20, and Co=$2, CR=20/(20+2) \\(\\approx\\) 0.9091 \n\n\n\n# Constants\nP = 22  # Selling price per book\nCo = 2   # Cost of production per book (real cost outlay)\nW = 14  # Wholesale price per book (internal transfer, not considered here)\n\n# Given values\nmu = 20000  # Approximate mean from data\nsigma = 2000  # Standard deviation from data\nz = 1.34  # CR\n\n# Calculate the optimal order quantity.\nQ_print = mu + z * sigma\n\nprofits = []\nfor demand in demand_data['Demand']:\n    sold_books = min(demand, Q_print)\n    unsold_books = max(Q_print - demand, 0)\n    \n    # Total profit calculation\n    profit = sold_books * (P - Co) - unsold_books * Co\n    profits.append(profit)\n\n# Average profit calculation\naverage_profit = sum(profits) / len(profits)\n(Q_print, average_profit)\n\n(22680.0, 393106.072)\n\n\nTherefore, based on the data, the supply chain should print 22,680 books and the supply chain average profit is approximately $393,106."
  },
  {
    "objectID": "projects/project2/456_HW1.html#question-1",
    "href": "projects/project2/456_HW1.html#question-1",
    "title": "456 Supply Chain HW 1",
    "section": "",
    "text": "Consider that the publisher, Scribner, acquired your bookstore, BN, i.e., you manage the whole supply chain, both the Publisher and the bookstore. In this case, what is the cost of understocking, the cost of overstocking, and the target service level of the supply chain (Note that the supply chain here means the Publisher and the bookstore)? How many books the supply chain should print? In this case, what is the supply chain average profit based on the data?\n\ncost of understocking(cu)\n\nThe lost profit for each book not sold when there is demand is essentially the retail price minus the wholesale price (since if we had the book, we could have sold it at retail price and replenished at wholesale price).\nCu=Retail Price−Wholesale Price=$22−$2=$20 per book\n\ncost of overstocking(co)\n\nThe cost of each unsold book is the cost of printing because once you’ve purchased the book, if not sold, the loss is what was paid to print it (considering you manage both the printing/publishing and the selling as one entity now).\nCo=Cost to Publisher to print =$2 per book\n\nTarget Service Level (Service Factor)\n\nThe target service level determines the probability that there will be no stockouts, which depends on how the costs of overstocking and understocking relate. The critical ratio (CR) is used to determine the target service level and is defined as: \\[(\\frac{Cu}{Cu+Co})\\]\nhere, Cu=$20, and Co=$2, CR=20/(20+2) \\(\\approx\\) 0.9091 \n\n\n\n# Constants\nP = 22  # Selling price per book\nCo = 2   # Cost of production per book (real cost outlay)\nW = 14  # Wholesale price per book (internal transfer, not considered here)\n\n# Given values\nmu = 20000  # Approximate mean from data\nsigma = 2000  # Standard deviation from data\nz = 1.34  # CR\n\n# Calculate the optimal order quantity.\nQ_print = mu + z * sigma\n\nprofits = []\nfor demand in demand_data['Demand']:\n    sold_books = min(demand, Q_print)\n    unsold_books = max(Q_print - demand, 0)\n    \n    # Total profit calculation\n    profit = sold_books * (P - Co) - unsold_books * Co\n    profits.append(profit)\n\n# Average profit calculation\naverage_profit = sum(profits) / len(profits)\n(Q_print, average_profit)\n\n(22680.0, 393106.072)\n\n\nTherefore, based on the data, the supply chain should print 22,680 books and the supply chain average profit is approximately $393,106."
  },
  {
    "objectID": "projects/project2/456_HW1.html#question-2",
    "href": "projects/project2/456_HW1.html#question-2",
    "title": "456 Supply Chain HW 1",
    "section": "Question 2",
    "text": "Question 2\nConsider the original case that we studied in class, i.e., the bookstore is BN, and the Publisher is a separate entity. You, as a bookstore owner, decides how many books to order considering the wholesale price and the buy-back price. We discussed the buy-back contract, in which the buy-back price was $12 in class.\nCan you find the pair of the wholesale price (note that the status-quo wholesale price is $14) and the buy-back price that will make both the bookstore and the publisher better off compared to the price pair of the wholesale price $14 and the buy-back price $12? As you change the wholesale price and the buy-back price, the order quantity of books will change.\n\nFirst, Calculating the average profits at $14 wholesale and $12 buy-back.\n\n\nimport scipy.stats as stats\n# Constants\nP = 22  # Selling price per book\nCo = 2  # Cost of overstocking by bookstore\nCu = 8  # Cost of understocking by bookstore\nbb = 12 # buy back price\nW = 14  # wholesale price\n# Given values\nmu = 20000  # Approximate mean from data\nsigma = 2000  # Standard deviation from data\nz_hc = stats.norm.ppf(Cu/(Co+Cu))  # CR\n\n# Calculate the optimal order quantity.\nQ = mu + z_hc * sigma\n\nprofits_hc = []\nprofits_bn = []\nfor demand in demand_data['Demand']:\n    sold_books = min(demand, Q)\n    unsold_books = max(Q - demand, 0)\n    \n    profit_hc = Q * (W-2) - unsold_books * bb\n    profit_bn = sold_books * P - 14 * Q + unsold_books * bb\n    profits_hc.append(profit_hc)\n    profits_bn.append(profit_bn)\n# Average profit calculation\naverage_profit_hc = sum(profits_hc) / len(profits_hc)\naverage_profit_bn = sum(profits_bn) / len(profits_bn)\n\nprint('average_profit_hc:$', average_profit_hc, 'average_profit_bn:$', average_profit_bn)\n\naverage_profit_hc:$ 237348.7659324771 average_profit_bn:$ 154424.15334277204\n\n\n\nOptimizing by using loops and printing out profits greater than $14 & $12 pair.\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\nP = 22  # Selling price per book\nmu = 20000  # Mean demand\nsigma = 2000  # Standard deviation of demand\n\n# Explore a range of wholesale (W) and buyback (bb) prices to maximize profits\nwholesale_prices = np.linspace(10, 15, 50)  # Range from $12 to $18\nbuyback_prices = np.linspace(9, 14, 50)    # Range from $5 to $12\n\n# Dictionary to store the results for easier comparison\nresult_summary = []\n\n# Calculate profits for each combination of W and bb\nfor W in wholesale_prices:\n    for bb in buyback_prices:\n        Cu = P-W  # Cost of understocking by bookstore\n        Co = W-bb  # Cost of overstocking by bookstore\n\n        z_hc = norm.ppf(Cu/(Cu + Co))\n        Q = mu + z_hc * sigma\n\n        profits_hc = []\n        profits_bn = []\n        for demand in demand_data['Demand']:\n            sold_books = min(demand, Q)\n            unsold_books = max(Q - demand, 0)\n\n            profit_hc = Q * (W - 2) - unsold_books * bb  # Whole chain profit\n            profit_bn = sold_books * P - W * Q + unsold_books * bb  # Bookstore profit\n\n            profits_hc.append(profit_hc)\n            profits_bn.append(profit_bn)\n\n        average_profit_hc = sum(profits_hc) / len(profits_hc)\n        average_profit_bn = sum(profits_bn) / len(profits_bn)\n        \n        result_summary.append((W, bb, average_profit_hc, average_profit_bn))\n\n# Convert results to DataFrame for better visualization\nresult_df = pd.DataFrame(result_summary, columns=['Wholesale Price', 'Buyback Price', 'Avg Profit HC', 'Avg Profit BN'])\n\npd.set_option('display.max_rows', 1000)\nprint(result_df[(result_df['Avg Profit HC'] &gt; 237349) & (result_df['Avg Profit BN'] &gt; 154425)])\n\n      Wholesale Price  Buyback Price  Avg Profit HC  Avg Profit BN\n2038        14.081633      12.877551  238252.145076  154542.234154\n2039        14.081633      12.979592  238093.692554  154794.936936\n2040        14.081633      13.081633  237914.915279  155059.251951\n2041        14.081633      13.183673  237711.375864  155335.807721\n2042        14.081633      13.285714  237476.984090  155625.209784\n2096        14.183673      13.693878  238492.020898  154516.500554\n2097        14.183673      13.795918  237995.610527  154859.478589"
  },
  {
    "objectID": "projects/project1/hw2_questions.html",
    "href": "projects/project1/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomers: Approximately 4.09 patents on average\nNon-Customers: Approximately 3.62 patents on average\nBoth distributions appear right-skewed, indicating that a majority of both customers and non-customers have a lower number of patents, with fewer entities holding a higher number of patents.\nThere is some overlap between the two distributions, but it is visible that customers tend to have slightly more patents on average.\n\n\n\n\n\n\n\n\n\n\n/var/folders/d6/rkl47jp12n963654hbtmzlkh0000gn/T/ipykernel_24452/2034835876.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='iscustomer', y='age', data=blue, palette='coolwarm')\n\n\n\n\n\n\n\n\n\n\nAs we can see in the plots, blueprints has more customers from Northeast region and has a average age lower than those who are not customers.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nThe log-likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\) is equal to \\(\\sum_{i=1}^n \\left(-\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\\)\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import factorial\nfrom scipy.optimize import minimize_scalar\ndef poisson_log_likelihood(lam, y):\n   return np.sum(-lam + y * np.log(lam) - np.log(factorial(y)))\n\n# Extract the 'patents' data\npatents = blue['patents'].values\n\n# Lambda values for which we want to compute the log-likelihood\nlambda_values = np.linspace(0.1, 10, 400)\nlog_likelihood_values = [poisson_log_likelihood(lam, patents) for lam in lambda_values]\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihood_values, label='Log-Likelihood')\nplt.xlabel('lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Distribution for Different lambda')\nplt.legend()\nplt.show()\n\n# Finding the MLE of lambda\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\" Negative log-likelihood for optimization purposes. \"\"\"\n    return -poisson_log_likelihood(lam, y)\n\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(patents,), method='bounded')\nlambda_mle = result.x\n\n\n\n\n\n\n\n\n\n\nThe MLE is approximately equal to 3.68 patents\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUsing sm.GLM\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\nimport statsmodels.api as sm\n\n# Checking using sm.GLM\n# Computing age squared\nblue['age_squared'] = blue['age'] ** 2\n\n# Convert 'region' into dummy variables\nencoder = OneHotEncoder(drop='first')  # Drop first to avoid dummy variable trap\nregion_dummies = encoder.fit_transform(blue[['region']]).toarray()  # Convert to dense array if needed\nregion_dummy_df = pd.DataFrame(region_dummies, columns=encoder.get_feature_names_out(['region']))\n\n# Joining the dummy variables with the original dataframe\ndata_prepared = blue.join(region_dummy_df)\n\n# Display the prepared data\ndata_prepared.head()\n\n# Defining the independent variables and the dependent variable\nX = data_prepared.drop(columns=['patents', 'region'])\ny = data_prepared['patents']\n\n# Adding a constant to the model (intercept)\nX = sm.add_constant(X)\n\n# Fitting a Poisson regression model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n\nCompute Manually\n\n\nCode\nfrom scipy.optimize import minimize\nimport numpy as np\nfrom scipy.special import gammaln\nfrom sklearn.preprocessing import StandardScaler\n\ndef poisson_log_likelihood(beta, X, y):\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n    log_likelihood = -np.sum(lambda_) + np.sum(y * eta - gammaln(y + 1))\n    return -log_likelihood\n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(poisson_log_likelihood, initial_beta, args=(X.values, y.values), method='BFGS')\n\n# Scaling the features (excluding the constant and binary/dummy variables)\nscaler = StandardScaler()\nX_scaled = X.copy()\nnumeric_features = ['age', 'age_squared']  # These are the numeric features we want to scale\nX_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])\n\ninitial_beta_scaled = np.zeros(X_scaled.shape[1])\nresult_scaled = minimize(poisson_log_likelihood, initial_beta_scaled, args=(X_scaled.values, y.values), method='BFGS')\n\n\nValues in () represents manual results\n\nAge: The coefficient is 0.1445(1.046), indicating that, holding other variables constant, each additional year of age is associated with a 14.45% increase in the expected count of patents.\nAge Squared: The coefficient is -0.0029(-1.141), suggesting that the effect of age on the patent count decreases at a rate proportional to the square of age. This represents the diminishing returns of age on patent counts.\nIs Customer: The coefficient is 0.1181(0.118), which means that being a customer of Blueprinty is associated with an 11.81% increase in the expected count of patents, controlling for other factors.\nRegion (Reference: Midwest):\nNortheast: Shows a positive coefficient of 0.0986(0.099), indicating a higher patent count compared to the Midwest.\nNorthwest: Shows a very slight decrease in log count of patents by -0.0201(-0.020) compared to the Midwest, though this is not statistically significant.\nSouth: Shows an increase by 0.0572(0.057) in log count, not statistically significant.\nSouthwest:Similar to South, shows a slight increase by 0.0513(0.051), also not statistically significant.\nBoth my manual and Python function results show being a customer of Blueprinty is associated with 12% increase in the expected count of patents. Thus, I concluded that Blueprinty’s software is very useful when it comes to patent application success chance."
  },
  {
    "objectID": "projects/project1/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project1/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomers: Approximately 4.09 patents on average\nNon-Customers: Approximately 3.62 patents on average\nBoth distributions appear right-skewed, indicating that a majority of both customers and non-customers have a lower number of patents, with fewer entities holding a higher number of patents.\nThere is some overlap between the two distributions, but it is visible that customers tend to have slightly more patents on average.\n\n\n\n\n\n\n\n\n\n\n/var/folders/d6/rkl47jp12n963654hbtmzlkh0000gn/T/ipykernel_24452/2034835876.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='iscustomer', y='age', data=blue, palette='coolwarm')\n\n\n\n\n\n\n\n\n\n\nAs we can see in the plots, blueprints has more customers from Northeast region and has a average age lower than those who are not customers.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nThe log-likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\) is equal to \\(\\sum_{i=1}^n \\left(-\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\\)\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import factorial\nfrom scipy.optimize import minimize_scalar\ndef poisson_log_likelihood(lam, y):\n   return np.sum(-lam + y * np.log(lam) - np.log(factorial(y)))\n\n# Extract the 'patents' data\npatents = blue['patents'].values\n\n# Lambda values for which we want to compute the log-likelihood\nlambda_values = np.linspace(0.1, 10, 400)\nlog_likelihood_values = [poisson_log_likelihood(lam, patents) for lam in lambda_values]\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihood_values, label='Log-Likelihood')\nplt.xlabel('lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Distribution for Different lambda')\nplt.legend()\nplt.show()\n\n# Finding the MLE of lambda\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\" Negative log-likelihood for optimization purposes. \"\"\"\n    return -poisson_log_likelihood(lam, y)\n\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(patents,), method='bounded')\nlambda_mle = result.x\n\n\n\n\n\n\n\n\n\n\nThe MLE is approximately equal to 3.68 patents\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUsing sm.GLM\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\nimport statsmodels.api as sm\n\n# Checking using sm.GLM\n# Computing age squared\nblue['age_squared'] = blue['age'] ** 2\n\n# Convert 'region' into dummy variables\nencoder = OneHotEncoder(drop='first')  # Drop first to avoid dummy variable trap\nregion_dummies = encoder.fit_transform(blue[['region']]).toarray()  # Convert to dense array if needed\nregion_dummy_df = pd.DataFrame(region_dummies, columns=encoder.get_feature_names_out(['region']))\n\n# Joining the dummy variables with the original dataframe\ndata_prepared = blue.join(region_dummy_df)\n\n# Display the prepared data\ndata_prepared.head()\n\n# Defining the independent variables and the dependent variable\nX = data_prepared.drop(columns=['patents', 'region'])\ny = data_prepared['patents']\n\n# Adding a constant to the model (intercept)\nX = sm.add_constant(X)\n\n# Fitting a Poisson regression model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n\nCompute Manually\n\n\nCode\nfrom scipy.optimize import minimize\nimport numpy as np\nfrom scipy.special import gammaln\nfrom sklearn.preprocessing import StandardScaler\n\ndef poisson_log_likelihood(beta, X, y):\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n    log_likelihood = -np.sum(lambda_) + np.sum(y * eta - gammaln(y + 1))\n    return -log_likelihood\n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(poisson_log_likelihood, initial_beta, args=(X.values, y.values), method='BFGS')\n\n# Scaling the features (excluding the constant and binary/dummy variables)\nscaler = StandardScaler()\nX_scaled = X.copy()\nnumeric_features = ['age', 'age_squared']  # These are the numeric features we want to scale\nX_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])\n\ninitial_beta_scaled = np.zeros(X_scaled.shape[1])\nresult_scaled = minimize(poisson_log_likelihood, initial_beta_scaled, args=(X_scaled.values, y.values), method='BFGS')\n\n\nValues in () represents manual results\n\nAge: The coefficient is 0.1445(1.046), indicating that, holding other variables constant, each additional year of age is associated with a 14.45% increase in the expected count of patents.\nAge Squared: The coefficient is -0.0029(-1.141), suggesting that the effect of age on the patent count decreases at a rate proportional to the square of age. This represents the diminishing returns of age on patent counts.\nIs Customer: The coefficient is 0.1181(0.118), which means that being a customer of Blueprinty is associated with an 11.81% increase in the expected count of patents, controlling for other factors.\nRegion (Reference: Midwest):\nNortheast: Shows a positive coefficient of 0.0986(0.099), indicating a higher patent count compared to the Midwest.\nNorthwest: Shows a very slight decrease in log count of patents by -0.0201(-0.020) compared to the Midwest, though this is not statistically significant.\nSouth: Shows an increase by 0.0572(0.057) in log count, not statistically significant.\nSouthwest:Similar to South, shows a slight increase by 0.0513(0.051), also not statistically significant.\nBoth my manual and Python function results show being a customer of Blueprinty is associated with 12% increase in the expected count of patents. Thus, I concluded that Blueprinty’s software is very useful when it comes to patent application success chance."
  },
  {
    "objectID": "projects/project1/hw2_questions.html#airbnb-case-study",
    "href": "projects/project1/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nPreparing the Data\n\n\nCode\nairbnb_data = pd.read_csv('data/airbnb.csv')\n# Drop the 'Unnamed: 0' as it is just an index column and convert date columns to datetime\nairbnb_data.drop(columns=['Unnamed: 0'], inplace=True)\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Handling missing values by imputation\n# For bathrooms and bedrooms, fill missing values with the median (common practice for count data)\n# For review scores, impute with the median (since they are ratings out of 10, median is a reasonable choice)\nmedian_values = {\n    'bathrooms': airbnb_data['bathrooms'].median(),\n    'bedrooms': airbnb_data['bedrooms'].median(),\n    'review_scores_cleanliness': airbnb_data['review_scores_cleanliness'].median(),\n    'review_scores_location': airbnb_data['review_scores_location'].median(),\n    'review_scores_value': airbnb_data['review_scores_value'].median()\n}\nairbnb_data.fillna(median_values, inplace=True)\n\n# Encoding categorical variables using one-hot encoding\nairbnb_data = pd.get_dummies(airbnb_data, columns=['room_type', 'instant_bookable'], drop_first=True)\n\n\nPlotting Number of Reviews\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Setting the aesthetics for the plots\nsns.set(style=\"whitegrid\")\n# Distribution of number of reviews\nplt.figure(figsize=(12, 6))\nsns.histplot(airbnb_data['number_of_reviews'], bins=30, kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWe see that the plot for number of reviews is heavily right skewed, indicating most of the listings have only a few reviews and listings with high number of reviews are on the more extreme side of this dataset.\n\nBuilding model\n\n\nCode\nimport statsmodels.api as sm\n\n# Convert all columns explicitly to float, except for the ones already identified as categorical, which will be integers\ncolumns_to_convert = ['days', 'bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness', \n                      'review_scores_location', 'review_scores_value']\n\nfor col in columns_to_convert:\n    airbnb_data[col] = airbnb_data[col].astype('float64')\n\n# Prepare the data for the model\nX = airbnb_data.drop(columns=['id', 'number_of_reviews', 'last_scraped', 'host_since'])\ny = airbnb_data['number_of_reviews']\n\n# Add a constant to the independent variable set\nX = sm.add_constant(X)\n\n# Attempt to fit the Poisson regression model again, converting X explicitly to ensure no 'object' dtype slips through\ntry:\n    X = X.astype('float64')\n    poisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n    model_summary = poisson_model.summary()\nexcept Exception as e:\n    model_summary = str(e)\n\n\n\nDays (days): Coefficient is positive (≈0.00005), indicating a small but positive association between the length of listing duration and the number of reviews. This implies that for each additional day a listing is available, the expected log count of reviews increases slightly.\nBathrooms (bathrooms): Coefficient is negative (−0.1161), suggesting that listings with more bathrooms tend to have slightly fewer reviews. This might indicate larger properties are booked less frequently.\nBedrooms (bedrooms): Coefficient is positive (0.0702), meaning that additional bedrooms are associated with an increase in the number of reviews, possibly indicating that larger capacity listings attract more guests/reviews.\nPrice (price): Coefficient is negative (−0.0002), indicating that higher prices are associated with fewer reviews, suggesting price sensitivity among bookers.\nReview Scores (Cleanliness, Location, Value): All have significant impacts. Higher cleanliness scores increase the review count (positive coefficient), whereas better location and value scores are associated with fewer reviews (negative coefficients), which might suggest that expectations are higher in well-located or good value properties, affecting review behaviors.\nRoom Type: Listings that are private rooms (−0.1457) or shared rooms (−0.4171) have fewer reviews compared to entire homes/apartments, with shared rooms showing a much stronger negative effect.\nInstant Bookable (instant_bookable_t): Having the listing available for instant booking is positively associated with the number of reviews (0.3468), suggesting that ease of booking contributes to more bookings and hence more reviews."
  }
]