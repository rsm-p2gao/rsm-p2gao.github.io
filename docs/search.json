[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brian Gao",
    "section": "",
    "text": "Welcome to my website!\nI’m a Business Analyst! !"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "ALCO case write up: Answer code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSegmentation Methods\n\n\n\n\n\n\nBrian Gao\n\n\nJun 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n456 Supply Chain HW 1\n\n\n\n\n\n\nBrian Gao\n\n\nJun 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHomework #3\n\n\n\n\n\n\nBrian Gao\n\n\nMay 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKey Drivers Analysis\n\n\n\n\n\n\nBrian Gao\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment_2b\n\n\n\n\n\n\nBrian Gao\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Examples\n\n\n\n\n\n\nBrian Gao\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nBrian Gao\n\n\nMay 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nBrian Gao\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Brian Gao’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project1/hw1_questions.html",
    "href": "projects/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#introduction",
    "href": "projects/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#data",
    "href": "projects/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\ndata = pd.read_stata('data/karlan_list_2007.dta')\n\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nNumber of months since last donation (mrm2)\n\n# Extracting the data for treatment and control groups\ntreatment_group = data[data['treatment'] == 1]['mrm2']\ncontrol_group = data[data['control'] == 1]['mrm2']\n\n# Dropping NA values for the t-test\ntreatment_group = treatment_group.dropna()\ncontrol_group = control_group.dropna()\n\n# Calculating the components of the t-statistic manually\nmean_treatment = treatment_group.mean()\nmean_control = control_group.mean()\nstd_treatment = treatment_group.std()\nstd_control = control_group.std()\nn_treatment = treatment_group.count()\nn_control = control_group.count()\n\n# Calculating the t-statistic using the formula\nnumerator = mean_treatment - mean_control\ndenominator = np.sqrt((std_treatment**2 / n_treatment) + (std_control**2 / n_control))\nt_statistic_manual = numerator / denominator\n\n# Performing the t-test using scipy's built-in t-test function for independent samples\nt_statistic_auto, p_value_auto = stats.ttest_ind(treatment_group, control_group, equal_var=False)\n\n# Comparing results from manual calculation and automatic function\n(t_statistic_manual, t_statistic_auto, p_value_auto)\n\nreg = rsm.model.regress(\n    data={\"data\": data},\n    rvar=\"mrm2\",\n    evar=\"treatment\"\n)\nreg.summary()\n\nLinear regression (OLS)\nData                 : data\nResponse variable    : mrm2\nExplanatory variables: treatment\nNull hyp.: the effect of x on mrm2 is zero\nAlt. hyp.: the effect of x on mrm2 is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       12.998      0.094  138.979  &lt; .001  ***\ntreatment        0.014      0.115    0.119   0.905     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.0\nF-statistic: 0.014 df(1, 50080), p.value 0.905\nNr obs: 50,082\n\n\nBy using t-test, I got the t-value for months since last donation approximately equal to 0.1195 and the corresponding p-value approximately equal to 0.9049 which we agree with the null hypothesis that the mean values for the treatment and control are equal for the mrm2 variable.\nThe coefficient for treatment is 0.014, suggesting that being in the treatment group is associated with an average increase in mrm2 of 0.014 units compared to the control group. However, this effect is not statistically significant (p = 0.905), implying that the treatment does not have a meaningful impact on mrm2. The t-value of 0.119 is far below any conventional critical value for significance (around ±1.96 for a 95% confidence level), reinforcing this finding. Lastly, the result is exactly the same compare to t-test.\nNumber of years since initial donation (years)\nBy using similar codes, I got the t-value for Number of years since initial donation approximately equal to -1.0909 and the corresponding p-value approximately equal to 0.2753 which we agree with the null hypothesis that the mean values for the treatment and control are equal for the years variable.\nThe coefficient for treatment is 0.009, suggesting that being in the treatment group is associated with an average increase in years of 0.009 units compared to the control group. However, this effect is not statistically significant (p = 0.27), implying that the treatment does not have a meaningful impact on years. The t-value of -1.103 is far from any conventional critical value for significance (around ±1.96 for a 95% confidence level), reinforcing this finding. Lastly, the result is slightly off compare to t-test.\nNumber of prior donations (freq)\nBy using similar codes, I got the t-value for Number of prior donations approximately equal to -0.1108 and the corresponding p-value approximately equal to 0.9117 which we agree with the null hypothesis that the mean values for the treatment and control are equal for the freq variable.\nThe coefficient for treatment is -0.012, suggesting that being in the treatment group is associated with an average decrease in freq of 0.012 units compared to the control group. However, this effect is not statistically significant (p = 0.912), implying that the treatment does not have a meaningful impact on freq. The t-value of -1.111 is far from any conventional critical value for significance (around ±1.96 for a 95% confidence level), reinforcing this finding. Lastly, the result is the same compare to t-test."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#experimental-results",
    "href": "projects/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\n\n\nt_statistic_manual: 3.2094621908279835 p-value: 0.0013309823450914173\nLinear regression (OLS)\nData                 : data\nResponse variable    : gave\nExplanatory variables: treatment\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\ntreatment        0.004      0.001    3.101   0.002   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 9.618 df(1, 50081), p.value 0.002\nNr obs: 50,083\n\n\nThe statistical tests performed — both the t-test and the bivariate linear regression — suggest that there is a statistically significant difference between the treatment and control groups regarding making a donation. The small p-values in both tests (t-test and regression) tell us that the observed differences in donation rates between the control and treatment groups are highly unlikely to have occurred by chance (p &lt; 0.05). This provides strong evidence in favor of the treatment’s impact on increasing donation rates. The results indicate that even small incentives or changes in how donations are solicited (like matching donations) can positively affect donor behavior. This aligns with behavioral economic theories suggesting that people are more likely to engage in pro-social behavior (like donating to charity) if they perceive their contribution as being more impactful or if they receive some form of positive reinforcement or matching contribution.\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 15 May 2024\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n11:33:14\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\nThese results match column 1 in table 3, confirming the positive and significant impact of treatment on the likelihood of making a charitable donation.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n1:1 vs 2:1 t_statistic: -0.965048975142932  p-value: 0.33453078237183076\n1:1 vs 3:1 t_statistic: -1.0150174470156275  p-value: 0.31010856527625774\n2:1 vs 3:1 t_statistic: -0.05011581369764474  p-value: 0.9600305476940865\n\n\nMy results support the “figures suggest” comment since my results also show that the ratios does not have a meaningful influence on behavior(p-value much higher than 0.05).\n\n\nLinear regression (OLS)\nData                 : data\nResponse variable    : gave\nExplanatory variables: ratio\nNull hyp.: the effect of x on gave is zero\nAlt. hyp.: the effect of x on gave is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept        0.018      0.001   16.225  &lt; .001  ***\nratio[1]         0.003      0.002    1.661   0.097    .\nratio[2]         0.005      0.002    2.744   0.006   **\nratio[3]         0.005      0.002    2.802   0.005   **\n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: 0.0\nF-statistic: 3.665 df(3, 50079), p.value 0.012\nNr obs: 50,083\n\n\nThe regression analysis assessing the impact of different match ratios on the likelihood of making a donation reveals that higher match ratios (2:1 and 3:1) significantly increase the probability of donating by approximately 0.5 percentage points compared to the baseline, a statistically significant finding with p-values of 0.006 and 0.005, respectively. Although the 1:1 match ratio also shows an increase in donation probability (by 0.3 percentage points), this effect is only marginally significant (p-value of 0.097).\n\n\ndirect_1:1 vs 2:1: 0.0019 direct_2:1 vs 3:1: 0.0001\ncoef_1:1 vs 2:1: 0.002 coef_2:1 vs 3:1: 0.0\n\n\nThe analysis reveals that increasing the match ratio from 1:1 to 2:1 enhances the probability of donation by approximately 0.188% (direct calculation) and predicts a 0.2% increase based on regression coefficients, indicating a statistically significant effect that suggests a higher match ratio effectively encourages more donations. However, further increasing the match ratio from 2:1 to 3:1 shows a negligible increase in donation likelihood (only 0.01% as per direct calculation) and no effect according to regression coefficients (0.0%), suggesting that beyond a 2:1 match, there are diminishing returns in terms of motivating additional donations. This pattern underscores the effectiveness of moderate increases in match ratios while highlighting a threshold beyond which higher ratios do not yield proportional gains in donor engagement.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nThe t-test indicates a relatively weak statistically significant difference in donation amounts between the treatment and control groups (p-value=0.055). By performing these analyses, we learn whether the treatment effectively increases donation amounts and quantifies the increase. This helps in understanding the financial impact of the treatment and can guide future decisions regarding the use of such treatments to boost donations.\nWhat happens if we limit the data to just people who made a donation and repeat the previous analysis?\n\n\nLinear regression (OLS)\nData                 : donors_data\nResponse variable    : amount\nExplanatory variables: treatment\nNull hyp.: the effect of x on amount is zero\nAlt. hyp.: the effect of x on amount is not zero\n\n           coefficient  std.error  t.value p.value     \nIntercept       45.540      2.423   18.792  &lt; .001  ***\ntreatment       -1.668      2.872   -0.581   0.561     \n\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-squared: 0.0, Adjusted R-squared: -0.001\nF-statistic: 0.337 df(1, 1032), p.value 0.561\nNr obs: 1,034\n\n\nOn average, individuals in the control group (or baseline category, assuming treatment = 0) donate $45.54. This is significantly different from zero (p &lt; .001), suggesting a high confidence in this average donation amount among the control group donors.\nBeing in the treatment group is associated with a decrease in the donation amount by $1.668 compared to the control group, although this effect is not statistically significant (p = 0.561). This implies that there is no strong evidence to suggest that treatment status influences the donation amount among those who chose to donate.\nIf the treatment was randomly assigned among participants, then the coefficient can be interpreted causally as the effect of the treatment on donation amounts among donors. This causal interpretation is valid under the assumption of random assignment."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#simulation-experiment",
    "href": "projects/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\n\nThese histograms will visually demonstrate the convergence property described by the Central Limit Theorem: as the sample size increases, the distribution of the sample mean (of differences, in this case) will approximate a normal distribution more closely, centering around the true mean difference (0.004) with decreasing variance. If zero is frequently not in the middle as sample sizes increase, it supports the hypothesis that the treatment does indeed have a real, positive effect on the likelihood of donations.\n\nimport pandas as pd\ndata = {\n    'Names': ['Alan', 'Bob', 'Charles'],\n    'Ages': [22, 33, 44]\n}\n\n# Create the DataFrame with an index\ndf = pd.DataFrame(data, index=['Data1', 'Data2', 'Data3'])\n\nprint(df)\n\n         Names  Ages\nData1     Alan    22\nData2      Bob    33\nData3  Charles    44"
  },
  {
    "objectID": "projects/project2/456_HW1.html",
    "href": "projects/project2/456_HW1.html",
    "title": "456 Supply Chain HW 1",
    "section": "",
    "text": "Consider that the publisher, Scribner, acquired your bookstore, BN, i.e., you manage the whole supply chain, both the Publisher and the bookstore. In this case, what is the cost of understocking, the cost of overstocking, and the target service level of the supply chain (Note that the supply chain here means the Publisher and the bookstore)? How many books the supply chain should print? In this case, what is the supply chain average profit based on the data?\n\ncost of understocking(cu)\n\nThe lost profit for each book not sold when there is demand is essentially the retail price minus the wholesale price (since if we had the book, we could have sold it at retail price and replenished at wholesale price).\nCu=Retail Price−Wholesale Price=$22−$2=$20 per book\n\ncost of overstocking(co)\n\nThe cost of each unsold book is the cost of printing because once you’ve purchased the book, if not sold, the loss is what was paid to print it (considering you manage both the printing/publishing and the selling as one entity now).\nCo=Cost to Publisher to print =$2 per book\n\nTarget Service Level (Service Factor)\n\nThe target service level determines the probability that there will be no stockouts, which depends on how the costs of overstocking and understocking relate. The critical ratio (CR) is used to determine the target service level and is defined as: \\[(\\frac{Cu}{Cu+Co})\\]\nhere, Cu=$20, and Co=$2, CR=20/(20+2) \\(\\approx\\) 0.9091 \n\n\n\n# Constants\nP = 22  # Selling price per book\nCo = 2   # Cost of production per book (real cost outlay)\nW = 14  # Wholesale price per book (internal transfer, not considered here)\n\n# Given values\nmu = 20000  # Approximate mean from data\nsigma = 2000  # Standard deviation from data\nz = 1.34  # CR\n\n# Calculate the optimal order quantity.\nQ_print = mu + z * sigma\n\nprofits = []\nfor demand in demand_data['Demand']:\n    sold_books = min(demand, Q_print)\n    unsold_books = max(Q_print - demand, 0)\n    \n    # Total profit calculation\n    profit = sold_books * (P - Co) - unsold_books * Co\n    profits.append(profit)\n\n# Average profit calculation\naverage_profit = sum(profits) / len(profits)\n(Q_print, average_profit)\n\n(22680.0, 393106.072)\n\n\nTherefore, based on the data, the supply chain should print 22,680 books and the supply chain average profit is approximately $393,106."
  },
  {
    "objectID": "projects/project2/456_HW1.html#question-1",
    "href": "projects/project2/456_HW1.html#question-1",
    "title": "456 Supply Chain HW 1",
    "section": "",
    "text": "Consider that the publisher, Scribner, acquired your bookstore, BN, i.e., you manage the whole supply chain, both the Publisher and the bookstore. In this case, what is the cost of understocking, the cost of overstocking, and the target service level of the supply chain (Note that the supply chain here means the Publisher and the bookstore)? How many books the supply chain should print? In this case, what is the supply chain average profit based on the data?\n\ncost of understocking(cu)\n\nThe lost profit for each book not sold when there is demand is essentially the retail price minus the wholesale price (since if we had the book, we could have sold it at retail price and replenished at wholesale price).\nCu=Retail Price−Wholesale Price=$22−$2=$20 per book\n\ncost of overstocking(co)\n\nThe cost of each unsold book is the cost of printing because once you’ve purchased the book, if not sold, the loss is what was paid to print it (considering you manage both the printing/publishing and the selling as one entity now).\nCo=Cost to Publisher to print =$2 per book\n\nTarget Service Level (Service Factor)\n\nThe target service level determines the probability that there will be no stockouts, which depends on how the costs of overstocking and understocking relate. The critical ratio (CR) is used to determine the target service level and is defined as: \\[(\\frac{Cu}{Cu+Co})\\]\nhere, Cu=$20, and Co=$2, CR=20/(20+2) \\(\\approx\\) 0.9091 \n\n\n\n# Constants\nP = 22  # Selling price per book\nCo = 2   # Cost of production per book (real cost outlay)\nW = 14  # Wholesale price per book (internal transfer, not considered here)\n\n# Given values\nmu = 20000  # Approximate mean from data\nsigma = 2000  # Standard deviation from data\nz = 1.34  # CR\n\n# Calculate the optimal order quantity.\nQ_print = mu + z * sigma\n\nprofits = []\nfor demand in demand_data['Demand']:\n    sold_books = min(demand, Q_print)\n    unsold_books = max(Q_print - demand, 0)\n    \n    # Total profit calculation\n    profit = sold_books * (P - Co) - unsold_books * Co\n    profits.append(profit)\n\n# Average profit calculation\naverage_profit = sum(profits) / len(profits)\n(Q_print, average_profit)\n\n(22680.0, 393106.072)\n\n\nTherefore, based on the data, the supply chain should print 22,680 books and the supply chain average profit is approximately $393,106."
  },
  {
    "objectID": "projects/project2/456_HW1.html#question-2",
    "href": "projects/project2/456_HW1.html#question-2",
    "title": "456 Supply Chain HW 1",
    "section": "Question 2",
    "text": "Question 2\nConsider the original case that we studied in class, i.e., the bookstore is BN, and the Publisher is a separate entity. You, as a bookstore owner, decides how many books to order considering the wholesale price and the buy-back price. We discussed the buy-back contract, in which the buy-back price was $12 in class.\nCan you find the pair of the wholesale price (note that the status-quo wholesale price is $14) and the buy-back price that will make both the bookstore and the publisher better off compared to the price pair of the wholesale price $14 and the buy-back price $12? As you change the wholesale price and the buy-back price, the order quantity of books will change.\n\nFirst, Calculating the average profits at $14 wholesale and $12 buy-back.\n\n\nimport scipy.stats as stats\n# Constants\nP = 22  # Selling price per book\nCo = 2  # Cost of overstocking by bookstore\nCu = 8  # Cost of understocking by bookstore\nbb = 12 # buy back price\nW = 14  # wholesale price\n# Given values\nmu = 20000  # Approximate mean from data\nsigma = 2000  # Standard deviation from data\nz_hc = stats.norm.ppf(Cu/(Co+Cu))  # CR\n\n# Calculate the optimal order quantity.\nQ = mu + z_hc * sigma\n\nprofits_hc = []\nprofits_bn = []\nfor demand in demand_data['Demand']:\n    sold_books = min(demand, Q)\n    unsold_books = max(Q - demand, 0)\n    \n    profit_hc = Q * (W-2) - unsold_books * bb\n    profit_bn = sold_books * P - 14 * Q + unsold_books * bb\n    profits_hc.append(profit_hc)\n    profits_bn.append(profit_bn)\n# Average profit calculation\naverage_profit_hc = sum(profits_hc) / len(profits_hc)\naverage_profit_bn = sum(profits_bn) / len(profits_bn)\n\nprint('average_profit_hc:$', average_profit_hc, 'average_profit_bn:$', average_profit_bn)\n\naverage_profit_hc:$ 237348.7659324771 average_profit_bn:$ 154424.15334277204\n\n\n\nOptimizing by using loops and printing out profits greater than $14 & $12 pair.\n\n\nimport numpy as np\nfrom scipy.stats import norm\n\nP = 22  # Selling price per book\nmu = 20000  # Mean demand\nsigma = 2000  # Standard deviation of demand\n\n# Explore a range of wholesale (W) and buyback (bb) prices to maximize profits\nwholesale_prices = np.linspace(10, 15, 50)  # Range from $12 to $18\nbuyback_prices = np.linspace(9, 14, 50)    # Range from $5 to $12\n\n# Dictionary to store the results for easier comparison\nresult_summary = []\n\n# Calculate profits for each combination of W and bb\nfor W in wholesale_prices:\n    for bb in buyback_prices:\n        Cu = P-W  # Cost of understocking by bookstore\n        Co = W-bb  # Cost of overstocking by bookstore\n\n        z_hc = norm.ppf(Cu/(Cu + Co))\n        Q = mu + z_hc * sigma\n\n        profits_hc = []\n        profits_bn = []\n        for demand in demand_data['Demand']:\n            sold_books = min(demand, Q)\n            unsold_books = max(Q - demand, 0)\n\n            profit_hc = Q * (W - 2) - unsold_books * bb  # Whole chain profit\n            profit_bn = sold_books * P - W * Q + unsold_books * bb  # Bookstore profit\n\n            profits_hc.append(profit_hc)\n            profits_bn.append(profit_bn)\n\n        average_profit_hc = sum(profits_hc) / len(profits_hc)\n        average_profit_bn = sum(profits_bn) / len(profits_bn)\n        \n        result_summary.append((W, bb, average_profit_hc, average_profit_bn))\n\n# Convert results to DataFrame for better visualization\nresult_df = pd.DataFrame(result_summary, columns=['Wholesale Price', 'Buyback Price', 'Avg Profit HC', 'Avg Profit BN'])\n\npd.set_option('display.max_rows', 1000)\nprint(result_df[(result_df['Avg Profit HC'] &gt; 237349) & (result_df['Avg Profit BN'] &gt; 154425)])\n\n      Wholesale Price  Buyback Price  Avg Profit HC  Avg Profit BN\n2038        14.081633      12.877551  238252.145076  154542.234154\n2039        14.081633      12.979592  238093.692554  154794.936936\n2040        14.081633      13.081633  237914.915279  155059.251951\n2041        14.081633      13.183673  237711.375864  155335.807721\n2042        14.081633      13.285714  237476.984090  155625.209784\n2096        14.183673      13.693878  238492.020898  154516.500554\n2097        14.183673      13.795918  237995.610527  154859.478589"
  },
  {
    "objectID": "projects/project1/hw2_questions.html",
    "href": "projects/project1/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomers: Approximately 4.09 patents on average\nNon-Customers: Approximately 3.62 patents on average\nBoth distributions appear right-skewed, indicating that a majority of both customers and non-customers have a lower number of patents, with fewer entities holding a higher number of patents.\nThere is some overlap between the two distributions, but it is visible that customers tend to have slightly more patents on average.\n\n\n\n\n\n\n\n\n\n\n/var/folders/d6/rkl47jp12n963654hbtmzlkh0000gn/T/ipykernel_26689/2034835876.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='iscustomer', y='age', data=blue, palette='coolwarm')\n\n\n\n\n\n\n\n\n\n\nAs we can see in the plots, blueprints has more customers from Northeast region and has a average age lower than those who are not customers.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nThe log-likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\) is equal to \\(\\sum_{i=1}^n \\left(-\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\\)\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import factorial\nfrom scipy.optimize import minimize_scalar\ndef poisson_log_likelihood(lam, y):\n   return np.sum(-lam + y * np.log(lam) - np.log(factorial(y)))\n\n# Extract the 'patents' data\npatents = blue['patents'].values\n\n# Lambda values for which we want to compute the log-likelihood\nlambda_values = np.linspace(0.1, 10, 400)\nlog_likelihood_values = [poisson_log_likelihood(lam, patents) for lam in lambda_values]\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihood_values, label='Log-Likelihood')\nplt.xlabel('lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Distribution for Different lambda')\nplt.legend()\nplt.show()\n\n# Finding the MLE of lambda\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\" Negative log-likelihood for optimization purposes. \"\"\"\n    return -poisson_log_likelihood(lam, y)\n\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(patents,), method='bounded')\nlambda_mle = result.x\n\n\n\n\n\n\n\n\n\n\nThe MLE is approximately equal to 3.68 patents\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUsing sm.GLM\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\nimport statsmodels.api as sm\n\n# Checking using sm.GLM\n# Computing age squared\nblue['age_squared'] = blue['age'] ** 2\n\n# Convert 'region' into dummy variables\nencoder = OneHotEncoder(drop='first')  # Drop first to avoid dummy variable trap\nregion_dummies = encoder.fit_transform(blue[['region']]).toarray()  # Convert to dense array if needed\nregion_dummy_df = pd.DataFrame(region_dummies, columns=encoder.get_feature_names_out(['region']))\n\n# Joining the dummy variables with the original dataframe\ndata_prepared = blue.join(region_dummy_df)\n\n# Display the prepared data\ndata_prepared.head()\n\n# Defining the independent variables and the dependent variable\nX = data_prepared.drop(columns=['patents', 'region'])\ny = data_prepared['patents']\n\n# Adding a constant to the model (intercept)\nX = sm.add_constant(X)\n\n# Fitting a Poisson regression model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n\nCompute Manually\n\n\nCode\nfrom scipy.optimize import minimize\nimport numpy as np\nfrom scipy.special import gammaln\nfrom sklearn.preprocessing import StandardScaler\n\ndef poisson_log_likelihood(beta, X, y):\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n    log_likelihood = -np.sum(lambda_) + np.sum(y * eta - gammaln(y + 1))\n    return -log_likelihood\n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(poisson_log_likelihood, initial_beta, args=(X.values, y.values), method='BFGS')\n\n# Scaling the features (excluding the constant and binary/dummy variables)\nscaler = StandardScaler()\nX_scaled = X.copy()\nnumeric_features = ['age', 'age_squared']  # These are the numeric features we want to scale\nX_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])\n\ninitial_beta_scaled = np.zeros(X_scaled.shape[1])\nresult_scaled = minimize(poisson_log_likelihood, initial_beta_scaled, args=(X_scaled.values, y.values), method='BFGS')\n\n\nValues in () represents manual results\n\nAge: The coefficient is 0.1445(1.046), indicating that, holding other variables constant, each additional year of age is associated with a 14.45% increase in the expected count of patents.\nAge Squared: The coefficient is -0.0029(-1.141), suggesting that the effect of age on the patent count decreases at a rate proportional to the square of age. This represents the diminishing returns of age on patent counts.\nIs Customer: The coefficient is 0.1181(0.118), which means that being a customer of Blueprinty is associated with an 11.81% increase in the expected count of patents, controlling for other factors.\nRegion (Reference: Midwest):\nNortheast: Shows a positive coefficient of 0.0986(0.099), indicating a higher patent count compared to the Midwest.\nNorthwest: Shows a very slight decrease in log count of patents by -0.0201(-0.020) compared to the Midwest, though this is not statistically significant.\nSouth: Shows an increase by 0.0572(0.057) in log count, not statistically significant.\nSouthwest:Similar to South, shows a slight increase by 0.0513(0.051), also not statistically significant.\nBoth my manual and Python function results show being a customer of Blueprinty is associated with 12% increase in the expected count of patents. Thus, I concluded that Blueprinty’s software is very useful when it comes to patent application success chance."
  },
  {
    "objectID": "projects/project1/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project1/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomers: Approximately 4.09 patents on average\nNon-Customers: Approximately 3.62 patents on average\nBoth distributions appear right-skewed, indicating that a majority of both customers and non-customers have a lower number of patents, with fewer entities holding a higher number of patents.\nThere is some overlap between the two distributions, but it is visible that customers tend to have slightly more patents on average.\n\n\n\n\n\n\n\n\n\n\n/var/folders/d6/rkl47jp12n963654hbtmzlkh0000gn/T/ipykernel_26689/2034835876.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='iscustomer', y='age', data=blue, palette='coolwarm')\n\n\n\n\n\n\n\n\n\n\nAs we can see in the plots, blueprints has more customers from Northeast region and has a average age lower than those who are not customers.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nThe log-likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\) is equal to \\(\\sum_{i=1}^n \\left(-\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\\)\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import factorial\nfrom scipy.optimize import minimize_scalar\ndef poisson_log_likelihood(lam, y):\n   return np.sum(-lam + y * np.log(lam) - np.log(factorial(y)))\n\n# Extract the 'patents' data\npatents = blue['patents'].values\n\n# Lambda values for which we want to compute the log-likelihood\nlambda_values = np.linspace(0.1, 10, 400)\nlog_likelihood_values = [poisson_log_likelihood(lam, patents) for lam in lambda_values]\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihood_values, label='Log-Likelihood')\nplt.xlabel('lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Distribution for Different lambda')\nplt.legend()\nplt.show()\n\n# Finding the MLE of lambda\ndef neg_poisson_log_likelihood(lam, y):\n    \"\"\" Negative log-likelihood for optimization purposes. \"\"\"\n    return -poisson_log_likelihood(lam, y)\n\nresult = minimize_scalar(neg_poisson_log_likelihood, bounds=(0.1, 10), args=(patents,), method='bounded')\nlambda_mle = result.x\n\n\n\n\n\n\n\n\n\n\nThe MLE is approximately equal to 3.68 patents\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUsing sm.GLM\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\nimport statsmodels.api as sm\n\n# Checking using sm.GLM\n# Computing age squared\nblue['age_squared'] = blue['age'] ** 2\n\n# Convert 'region' into dummy variables\nencoder = OneHotEncoder(drop='first')  # Drop first to avoid dummy variable trap\nregion_dummies = encoder.fit_transform(blue[['region']]).toarray()  # Convert to dense array if needed\nregion_dummy_df = pd.DataFrame(region_dummies, columns=encoder.get_feature_names_out(['region']))\n\n# Joining the dummy variables with the original dataframe\ndata_prepared = blue.join(region_dummy_df)\n\n# Display the prepared data\ndata_prepared.head()\n\n# Defining the independent variables and the dependent variable\nX = data_prepared.drop(columns=['patents', 'region'])\ny = data_prepared['patents']\n\n# Adding a constant to the model (intercept)\nX = sm.add_constant(X)\n\n# Fitting a Poisson regression model\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\n\nCompute Manually\n\n\nCode\nfrom scipy.optimize import minimize\nimport numpy as np\nfrom scipy.special import gammaln\nfrom sklearn.preprocessing import StandardScaler\n\ndef poisson_log_likelihood(beta, X, y):\n    eta = X @ beta\n    lambda_ = np.exp(eta)\n    log_likelihood = -np.sum(lambda_) + np.sum(y * eta - gammaln(y + 1))\n    return -log_likelihood\n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(poisson_log_likelihood, initial_beta, args=(X.values, y.values), method='BFGS')\n\n# Scaling the features (excluding the constant and binary/dummy variables)\nscaler = StandardScaler()\nX_scaled = X.copy()\nnumeric_features = ['age', 'age_squared']  # These are the numeric features we want to scale\nX_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])\n\ninitial_beta_scaled = np.zeros(X_scaled.shape[1])\nresult_scaled = minimize(poisson_log_likelihood, initial_beta_scaled, args=(X_scaled.values, y.values), method='BFGS')\n\n\nValues in () represents manual results\n\nAge: The coefficient is 0.1445(1.046), indicating that, holding other variables constant, each additional year of age is associated with a 14.45% increase in the expected count of patents.\nAge Squared: The coefficient is -0.0029(-1.141), suggesting that the effect of age on the patent count decreases at a rate proportional to the square of age. This represents the diminishing returns of age on patent counts.\nIs Customer: The coefficient is 0.1181(0.118), which means that being a customer of Blueprinty is associated with an 11.81% increase in the expected count of patents, controlling for other factors.\nRegion (Reference: Midwest):\nNortheast: Shows a positive coefficient of 0.0986(0.099), indicating a higher patent count compared to the Midwest.\nNorthwest: Shows a very slight decrease in log count of patents by -0.0201(-0.020) compared to the Midwest, though this is not statistically significant.\nSouth: Shows an increase by 0.0572(0.057) in log count, not statistically significant.\nSouthwest:Similar to South, shows a slight increase by 0.0513(0.051), also not statistically significant.\nBoth my manual and Python function results show being a customer of Blueprinty is associated with 12% increase in the expected count of patents. Thus, I concluded that Blueprinty’s software is very useful when it comes to patent application success chance."
  },
  {
    "objectID": "projects/project1/hw2_questions.html#airbnb-case-study",
    "href": "projects/project1/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nPreparing the Data\n\n\nCode\nairbnb_data = pd.read_csv('data/airbnb.csv')\n# Drop the 'Unnamed: 0' as it is just an index column and convert date columns to datetime\nairbnb_data.drop(columns=['Unnamed: 0'], inplace=True)\nairbnb_data['last_scraped'] = pd.to_datetime(airbnb_data['last_scraped'])\nairbnb_data['host_since'] = pd.to_datetime(airbnb_data['host_since'])\n\n# Handling missing values by imputation\n# For bathrooms and bedrooms, fill missing values with the median (common practice for count data)\n# For review scores, impute with the median (since they are ratings out of 10, median is a reasonable choice)\nmedian_values = {\n    'bathrooms': airbnb_data['bathrooms'].median(),\n    'bedrooms': airbnb_data['bedrooms'].median(),\n    'review_scores_cleanliness': airbnb_data['review_scores_cleanliness'].median(),\n    'review_scores_location': airbnb_data['review_scores_location'].median(),\n    'review_scores_value': airbnb_data['review_scores_value'].median()\n}\nairbnb_data.fillna(median_values, inplace=True)\n\n# Encoding categorical variables using one-hot encoding\nairbnb_data = pd.get_dummies(airbnb_data, columns=['room_type', 'instant_bookable'], drop_first=True)\n\n\nPlotting Number of Reviews\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Setting the aesthetics for the plots\nsns.set(style=\"whitegrid\")\n# Distribution of number of reviews\nplt.figure(figsize=(12, 6))\nsns.histplot(airbnb_data['number_of_reviews'], bins=30, kde=True)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWe see that the plot for number of reviews is heavily right skewed, indicating most of the listings have only a few reviews and listings with high number of reviews are on the more extreme side of this dataset.\n\nBuilding model\n\n\nCode\nimport statsmodels.api as sm\n\n# Convert all columns explicitly to float, except for the ones already identified as categorical, which will be integers\ncolumns_to_convert = ['days', 'bathrooms', 'bedrooms', 'price', 'review_scores_cleanliness', \n                      'review_scores_location', 'review_scores_value']\n\nfor col in columns_to_convert:\n    airbnb_data[col] = airbnb_data[col].astype('float64')\n\n# Prepare the data for the model\nX = airbnb_data.drop(columns=['id', 'number_of_reviews', 'last_scraped', 'host_since'])\ny = airbnb_data['number_of_reviews']\n\n# Add a constant to the independent variable set\nX = sm.add_constant(X)\n\n# Attempt to fit the Poisson regression model again, converting X explicitly to ensure no 'object' dtype slips through\ntry:\n    X = X.astype('float64')\n    poisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n    model_summary = poisson_model.summary()\nexcept Exception as e:\n    model_summary = str(e)\n\n\n\nDays (days): Coefficient is positive (≈0.00005), indicating a small but positive association between the length of listing duration and the number of reviews. This implies that for each additional day a listing is available, the expected log count of reviews increases slightly.\nBathrooms (bathrooms): Coefficient is negative (−0.1161), suggesting that listings with more bathrooms tend to have slightly fewer reviews. This might indicate larger properties are booked less frequently.\nBedrooms (bedrooms): Coefficient is positive (0.0702), meaning that additional bedrooms are associated with an increase in the number of reviews, possibly indicating that larger capacity listings attract more guests/reviews.\nPrice (price): Coefficient is negative (−0.0002), indicating that higher prices are associated with fewer reviews, suggesting price sensitivity among bookers.\nReview Scores (Cleanliness, Location, Value): All have significant impacts. Higher cleanliness scores increase the review count (positive coefficient), whereas better location and value scores are associated with fewer reviews (negative coefficients), which might suggest that expectations are higher in well-located or good value properties, affecting review behaviors.\nRoom Type: Listings that are private rooms (−0.1457) or shared rooms (−0.4171) have fewer reviews compared to entire homes/apartments, with shared rooms showing a much stronger negative effect.\nInstant Bookable (instant_bookable_t): Having the listing available for instant booking is positively associated with the number of reviews (0.3468), suggesting that ease of booking contributes to more bookings and hence more reviews."
  },
  {
    "objectID": "projects/project3/hw3_questions.html",
    "href": "projects/project3/hw3_questions.html",
    "title": "Multinomial Logit Examples",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/project3/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/project3/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Multinomial Logit Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\nWe will use the yogurt_data dataset, which provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc.\n\n\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\nid: An anonymized consumer identifier.\ny1, y2, y3, y4: Binary vectors indicating which yogurt product (1 through 4) was chosen by the consumer.\nf1, f2, f3, f4: Binary vectors indicating if yogurt products (1 through 4) were featured/advertised in the store at the time of purchase.\np1, p2, p3, p4: Prices of yogurt products (1 through 4) per ounce.\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\n\n\nCode\nimport numpy as np\n\n# Reshape the data from wide to long format for features and prices\nfeatures_long = pd.melt(data, id_vars=['id'], value_vars=['f1', 'f2', 'f3', 'f4'],\n                        var_name='feature', value_name='X_f')\nfeatures_long['product'] = features_long['feature'].str.extract(r'(\\d)').astype(int)\n\nprices_long = pd.melt(data, id_vars=['id'], value_vars=['p1', 'p2', 'p3', 'p4'],\n                      var_name='price', value_name='X_p')\nprices_long['product'] = prices_long['price'].str.extract(r'(\\d)').astype(int)\n\n# Merge features and prices\ndf_long = pd.merge(features_long[['id', 'product', 'X_f']], prices_long[['id', 'product', 'X_p']], on=['id', 'product'])\n\n# Create binary indicators for each product\ndf_long['Yogurt1'] = (df_long['product'] == 1).astype(int)\ndf_long['Yogurt2'] = (df_long['product'] == 2).astype(int)\ndf_long['Yogurt3'] = (df_long['product'] == 3).astype(int)\n\n# Create the dependent variable y (product choice) in long format\nchoices_long = pd.melt(data, id_vars=['id'], value_vars=['y1', 'y2', 'y3', 'y4'],\n                       var_name='chosen', value_name='choice')\nchoices_long['product'] = choices_long['chosen'].str.extract(r'(\\d)').astype(int)\n\n# Merge with the main dataframe to get the choice indicator\ndf_long = pd.merge(df_long, choices_long[['id', 'product', 'choice']], on=['id', 'product'], how='left')\n\n# Display the prepared data\ndf_long.head()\n\n\n\n\n\n\n\n\n\n\nid\nproduct\nX_f\nX_p\nYogurt1\nYogurt2\nYogurt3\nchoice\n\n\n\n\n0\n1\n1\n0\n0.108\n1\n0\n0\n0\n\n\n1\n2\n1\n0\n0.108\n1\n0\n0\n0\n\n\n2\n3\n1\n0\n0.108\n1\n0\n0\n0\n\n\n3\n4\n1\n0\n0.108\n1\n0\n0\n0\n\n\n4\n5\n1\n0\n0.125\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\nEstimation\nThe log-likelihood function is \\(\\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} x_j' \\beta - \\sum_{i=1}^n \\log \\left( \\sum_{k=1}^J e^{x_k' \\beta} \\right)\\)\n\n\nCode\nimport statsmodels.api as sm\n# Prepare the independent variables (X)\nindependent_vars = ['Yogurt1', 'Yogurt2', 'Yogurt3', 'X_f', 'X_p']\nX = df_long[independent_vars]\n\n# Add intercept\nX = sm.add_constant(X)\n\n# Prepare the dependent variable (y)\ny = df_long['choice']\n\n# Fit the multinomial logit model\nmodel = sm.MNLogit(y, X)\nresult = model.fit()\n\n# Print the results\nresult.summary()\n\n\nOptimization terminated successfully.\n         Current function value: 0.477971\n         Iterations 7\n\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nchoice\nNo. Observations:\n9720\n\n\nModel:\nMNLogit\nDf Residuals:\n9714\n\n\nMethod:\nMLE\nDf Model:\n5\n\n\nDate:\nThu, 16 May 2024\nPseudo R-squ.:\n0.1500\n\n\nTime:\n11:59:26\nLog-Likelihood:\n-4645.9\n\n\nconverged:\nTrue\nLL-Null:\n-5465.9\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.000\n\n\n\n\n\n\nchoice=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n1.2830\n0.173\n7.420\n0.000\n0.944\n1.622\n\n\nYogurt1\n1.4179\n0.089\n16.008\n0.000\n1.244\n1.591\n\n\nYogurt2\n0.9013\n0.065\n13.909\n0.000\n0.774\n1.028\n\n\nYogurt3\n-3.1405\n0.142\n-22.072\n0.000\n-3.419\n-2.862\n\n\nX_f\n0.4714\n0.119\n3.959\n0.000\n0.238\n0.705\n\n\nX_p\n-31.9761\n2.089\n-15.305\n0.000\n-36.071\n-27.881\n\n\n\n\n\n\n\n\nDiscussion\nUnderstanding the Multinomial Logit Model The multinomial logit (MNL) model is a statistical tool for modeling consumer choice behavior among multiple alternatives. It assumes that each consumer selects the product that provides the highest utility, where the utility is a linear function of product characteristics and an error term that follows an extreme value distribution.\nBuilding and analyzing the MNL model provided several key insights:\nConsumer Preferences: The model identified the most and least preferred yogurts based on their baseline utilities.\nPrice Sensitivity: The negative price coefficient highlighted the importance of pricing in consumer decision-making.\nPredictive Power: The model’s ability to simulate counterfactual scenarios and predict changes in market shares underscores its utility for strategic decision-making in marketing and product management.\n\nMost Preferred Yogurt: The yogurt with the highest intercept is Yogurt 1 (intercept = 1.2830).\nLeast Preferred Yogurt: The yogurt with the lowest intercept is Yogurt 3 (intercept = -3.1405).\n\nThe estimated coefficient for the price variable is -31.9761. This coefficient tells us how much utility changes with a change in price.\nTo calculate the dollar benefit between the most preferred and the least preferred yogurt: Difference in Utility: Intercept of Yogurt 1−Intercept of Yogurt 3=1.2830−(−3.1405)=4.4235\nDollar Benefit=4.4235/31.9761≈0.1383 dollars per unit.\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\n\n\nCode\nimport numpy as np\n# Calculate the initial probabilities\ndef calculate_probabilities(X, model):\n    # Reshape model.params to match the dimensions for matrix multiplication\n    params = model.params.values.flatten()\n    linear_combination = np.dot(X, params)\n    exp_linear_combination = np.exp(linear_combination.reshape(-1, 4))\n    probabilities = exp_linear_combination / np.sum(exp_linear_combination, axis=1, keepdims=True)\n    return probabilities\n\n# Prepare the initial data\nX_initial = df_long[independent_vars]\nX_initial = sm.add_constant(X_initial)\n\n# Calculate initial probabilities\nprobabilities_initial = calculate_probabilities(X_initial, result)\n\n# Calculate initial market shares\ninitial_market_shares = np.mean(probabilities_initial, axis=0)\n\n# Simulate a counterfactual scenario: Increase the price of Yogurt 1 by $0.10\ndf_long_counterfactual = df_long.copy()\ndf_long_counterfactual.loc[df_long_counterfactual['product'] == 1, 'X_p'] += 0.10\n\n# Prepare the counterfactual data\nX_counterfactual = df_long_counterfactual[independent_vars]\nX_counterfactual = sm.add_constant(X_counterfactual)\n\n# Calculate counterfactual probabilities\nprobabilities_counterfactual = calculate_probabilities(X_counterfactual, result)\n\n# Calculate new market shares\nnew_market_shares = np.mean(probabilities_counterfactual, axis=0)\n\n# Print initial and new market shares\ninitial_market_shares, new_market_shares\n\n\n(array([0.25112971, 0.2468373 , 0.25305383, 0.24897917]),\n array([0.2510665 , 0.2467384 , 0.25315639, 0.24903872]))\n\n\n\nWhile the changes are minor, this suggests that increasing the price of Yogurt 1 slightly reduces its market share, and other yogurts see a corresponding small increase in their market shares. ## 2. Estimating Minivan Preferences\n\n\n\nData\n\n\nCode\n# Calculate the number of respondents\nnum_respondents = r_data['resp.id'].nunique()\n\n# Calculate the number of choice tasks per respondent\nnum_choice_tasks_per_respondent = r_data.groupby('resp.id')['ques'].nunique().mean()\n\n# Calculate the number of alternatives per choice task\nnum_alternatives_per_choice_task = r_data.groupby(['resp.id', 'ques']).size().mean()\n\n# Extract unique levels for each attribute\nattributes = {\n    \"number_of_seats\": r_data['seat'].unique().tolist(),\n    \"cargo_space\": r_data['cargo'].unique().tolist(),\n    \"engine_type\": r_data['eng'].unique().tolist(),\n    \"price\": r_data['price'].unique().tolist()\n}\n\n\nNumber of respondents: 200\nNumber of choice tasks per respondent: 15\nNumber of alternatives presented in each choice task: 3\nFor each alternative, the attributes and their levels were:\nNumber of seats: 6, 7, 8\nCargo space: 2ft, 3ft\nEngine type: Gas, Hybrid (Hyb), Electric (Elec)\nPrice: 30, 35, 40 (in thousands of dollars)\n\n\nModel\n\n\nCode\nimport statsmodels.api as sm\ndata_prepared = pd.get_dummies(r_data, columns=['seat', 'cargo', 'eng'], drop_first=True)\ndata_prepared = data_prepared.astype({\n    'seat_7': int,\n    'seat_8': int,\n    'cargo_3ft': int,\n    'eng_hyb': int,\n    'price': float,\n    'choice': int\n})\n\n# Define the independent variables (excluding '6 seats', '2ft cargo', and 'gas engine')\nX = data_prepared[['seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'price']]\n\n# Add a constant term for the intercept\nX = sm.add_constant(X)\n\n# Define the dependent variable\ny = data_prepared['choice']\n\n# Ensure data is converted to numpy arrays to avoid dtype issues\nX = np.asarray(X)\ny = np.asarray(y)\n\n# Fit the MNL model\nmnl_model = sm.MNLogit(y, X)\nmnl_results = mnl_model.fit()\n\n# Extract the coefficients and standard errors\ncoefficients = mnl_results.params\nstandard_errors = mnl_results.bse\n\n# Create a table of coefficients and standard errors\nresults_table = pd.DataFrame({\n    'Variable': ['const', 'seat_7', 'seat_8', 'cargo_3ft', 'eng_hyb', 'price'],\n    'Coefficient': coefficients.flatten(),\n    'Standard Error': standard_errors.flatten()\n})\n\nresults_table\n\n\nOptimization terminated successfully.\n         Current function value: 0.591151\n         Iterations 5\n\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStandard Error\n\n\n\n\n0\nconst\n4.514112\n0.209492\n\n\n1\nseat_7\n-0.483220\n0.057552\n\n\n2\nseat_8\n-0.264506\n0.056432\n\n\n3\ncargo_3ft\n0.409893\n0.046989\n\n\n4\neng_hyb\n-0.110226\n0.050042\n\n\n5\nprice\n-0.148564\n0.005948\n\n\n\n\n\n\n\n\n\n\nResults\nassume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market | Minivan | Seats | Cargo | Engine | Price | |———|——-|——-|——–|——-| | A | 7 | 2 | Hyb | 30 | | B | 6 | 2 | Gas | 30 | | C | 8 | 2 | Gas | 30 | | D | 7 | 3 | Gas | 40 | | E | 6 | 2 | Elec | 40 | | F | 7 | 2 | Hyb | 35 |\n\nInterpretation of Results: Intercept (const): The baseline utility when all other variables are at their reference levels (6 seats, 2ft cargo, gas engine).\n\nSeat: 7 (seat_7): Having 7 seats decreases the utility by 0.483 compared to the reference level of 6 seats.\nSeat: 8 (seat_8): Having 8 seats decreases the utility by 0.265 compared to the reference level of 6 seats.\nCargo: 3ft (cargo_3ft): Having 3ft cargo space increases the utility by 0.410 compared to the reference level of 2ft cargo space.\nEngine: Hybrid (eng_hyb): Having a hybrid engine decreases the utility by 0.110 compared to the reference level of a gas engine.\nPrice: For each additional thousand dollars in price, the utility decreases by 0.149.\n\nMost Preferred: 3ft of cargo space (positive coefficient).\nLess Preferred: 7 seats and 8 seats (negative coefficients), with 7 seats being the least preferred.\ndollar value:\n\nCargo: 3ft (cargo_3ft): 0.410\nPrice: -0.149\nDollar Value= 0.149/0.410≈2.75\nThis value is in thousands of dollars since the price coefficient was per thousand dollars.\n\nFinal Dollar Value = 2.75×1000=$2750\n\n\n\nCode\n# Define the coefficients from the MNL model\nbeta_const = 4.514\nbeta_seat_7 = -0.483\nbeta_seat_8 = -0.265\nbeta_cargo_3ft = 0.410\nbeta_eng_hyb = -0.110\nbeta_price = -0.149\n\n# Define the minivan attributes\nminivans = {\n    'A': {'seat': 7, 'cargo': 2, 'engine': 'hyb', 'price': 30},\n    'B': {'seat': 6, 'cargo': 2, 'engine': 'gas', 'price': 30},\n    'C': {'seat': 8, 'cargo': 2, 'engine': 'gas', 'price': 30},\n    'D': {'seat': 7, 'cargo': 3, 'engine': 'gas', 'price': 40},\n    'E': {'seat': 6, 'cargo': 2, 'engine': 'elec', 'price': 40},\n    'F': {'seat': 7, 'cargo': 2, 'engine': 'hyb', 'price': 35},\n}\n\n# Calculate utility for each minivan\nutilities = {}\nfor minivan, attrs in minivans.items():\n    utility = beta_const\n    if attrs['seat'] == 7:\n        utility += beta_seat_7\n    elif attrs['seat'] == 8:\n        utility += beta_seat_8\n    if attrs['cargo'] == 3:\n        utility += beta_cargo_3ft\n    if attrs['engine'] == 'hyb':\n        utility += beta_eng_hyb\n    # Electric engine coefficient is not provided; assume it to be 0\n    utility += beta_price * attrs['price']\n    utilities[minivan] = utility\n\n# Calculate exponential of utilities\nexp_utilities = {minivan: np.exp(utility) for minivan, utility in utilities.items()}\n\n# Sum of exponential utilities\ntotal_exp_utility = sum(exp_utilities.values())\n\n# Calculate market shares\nmarket_shares = {minivan: exp_utility / total_exp_utility for minivan, exp_utility in exp_utilities.items()}\n\n\nMinivan B(6 seats, 2ft cargo, gas engine, $30k) has the highest predicted market share at 33.14%.\nMinivan C (8 seats, 2ft cargo, gas engine, $30k) follows with 25.43%.\nMinivan A (7 seats, 2ft cargo, hybrid engine, $30k) has 18.32%.\nMinivan F (7 seats, 2ft cargo, hybrid engine, $35k) has 8.70%.\nMinivan E (6 seats, 2ft cargo, electric engine, $40k) has 7.47%.\nMinivan D (7 seats, 3ft cargo, gas engine, $40k) has the lowest predicted market share at 6.94%."
  },
  {
    "objectID": "projects/project3/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/project3/hw3_questions.html#estimating-minivan-preferences",
    "title": "Multinomial Logit Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\ntodo: download the dataset from here: http://goo.gl/5xQObB\ntodo: describe the data a bit. How many respondents took the conjoint survey? How many choice tasks did each respondent complete? How many alternatives were presented on each choice task? For each alternative.\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\nModel\ntodo: estimate a MNL model omitting the following levels to avoide multicollinearity (6 seats, 2ft cargo, and gas engine). Include price as a continuous variable. Show a table of coefficients and standard errors. You may use your own likelihood function from above, or you may use a function from a package/library to perform the estimation.\n\n\nResults\ntodo: Interpret the coefficients. Which features are more preferred?\ntodo: Use the price coefficient as a dollar-per-util conversion factor. What is the dollar value of 3ft of cargo space as compared to 2ft of cargo space?\ntodo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\nhint: this example is taken from the “R 4 Marketing Research” book by Chapman and Feit. I believe the same example is present in the companion book titled “Python 4 Marketing Research”. I encourage you to attempt these questions on your own, but if you get stuck or would like to compare you results to “the answers,” you may consult the Chapman and Feit books."
  },
  {
    "objectID": "projects/456_case/Assignment_2b.html",
    "href": "projects/456_case/Assignment_2b.html",
    "title": "Assignment_2b",
    "section": "",
    "text": "The cash-to-cash cycle (CCC) measures the time it takes for a company to convert its investments in inventory and other resources into cash flows from sales. It comprises the following components:\nDays Inventory Outstanding (DIO): The average number of days that a company holds inventory before selling it.\nDays Sales Outstanding (DSO): The average number of days it takes to collect payment after a sale has been made.\nDays Payable Outstanding (DPO): The average number of days a company takes to pay its suppliers.\n\n\\[\\begin{align*}\n    \\text{DIO} &= \\left( \\frac{\\text{Average Inventory}}{\\text{Cost of Goods Sold}} \\right) \\times 365 \\\\\n    \\text{DSO} &= \\left( \\frac{\\text{Average Accounts Receivable}}{\\text{Net Sales}} \\right) \\times 365 \\\\\n    \\text{DPO} &= \\left( \\frac{\\text{Average Accounts Payable}}{\\text{Cost of Goods Sold}} \\right) \\times 365 \\\\\n    \\text{CCC} &= \\text{DIO} + \\text{DSO} - \\text{DPO}\n\\end{align*}\\]\n\n\nDays Inventory Outstanding (DIO):\nAverage Inventory (2023): \\(\\frac{63,843 + 56,481}{2} = 60,162\\)\nCost of Goods Sold (2023): $466,549 million\nDIO = \\(\\left( \\frac{60,162}{466,549} \\right) \\times 365 = 47.07\\) days\nDays Sales Outstanding (DSO)\nAverage Accounts Receivable (2023): \\(\\frac{3,519 + 3,094}{2} = 3,307\\)\nNet Sales (2023): $611,289 million\nDSO = \\(\\left( \\frac{3,307}{611,289} \\right) \\times 365 = 1.97\\) days\nDays Payable Outstanding (DPO)\nAverage Accounts Payable (2023): \\(\\frac{50,257 + 46,092}{2} = 48,175\\)\nCost of Goods Sold (2023): $466,549 million\nDPO = \\(\\left( \\frac{48,175}{466,549} \\right) \\times 365 = 37.69\\) days\nCash-to-Cash Cycle (CCC)\nCCC = \\(47.07 + 1.97 - 37.69 = 11.35\\) days\n\n\n\nDays Inventory Outstanding (DIO)\nAverage Inventory (2023): \\(\\frac{17,907 + 16,651}{2} = 17,279\\)\nCost of Goods Sold (2023): $212,586 million\nDIO = \\(\\left( \\frac{17,279}{212,586} \\right) \\times 365 = 29.70\\) days\nDays Sales Outstanding (DSO)\nAverage Accounts Receivable (2023): \\(\\frac{2,241 + 2,285}{2} = 2,263\\)\nNet Sales (2023): $237,710 million\nDSO = \\(\\left( \\frac{2,263}{237,710} \\right) \\times 365 = 3.47\\) days\nDays Payable Outstanding (DPO)\nAverage Accounts Payable (2023): \\(\\frac{14,838 + 14,903}{2} = 14,871\\)\nCost of Goods Sold (2023): $212,586 million\nDPO = \\(\\left( \\frac{14,871}{212,586} \\right) \\times 365 = 25.52\\) days\nCash-to-Cash Cycle (CCC)\nC2C = \\(29.70 + 3.47 - 25.52 = 7.65\\) days\n\n\n\nWalmart: 11.35 days\nCostco: 7.65 days\n\n\n\nCostco has a shorter cash-to-cash cycle (7.65 days) compared to Walmart (11.35 days), indicating Costco converts its inventory and resources into cash more quickly than Walmart. This efficiency could be attributed to Costco’s business model of rapid inventory turnover and efficient inventory management."
  },
  {
    "objectID": "projects/456_case/Assignment_2b.html#introduction",
    "href": "projects/456_case/Assignment_2b.html#introduction",
    "title": "Assignment_2b",
    "section": "",
    "text": "The cash-to-cash cycle (CCC) measures the time it takes for a company to convert its investments in inventory and other resources into cash flows from sales. It comprises the following components:\nDays Inventory Outstanding (DIO): The average number of days that a company holds inventory before selling it.\nDays Sales Outstanding (DSO): The average number of days it takes to collect payment after a sale has been made.\nDays Payable Outstanding (DPO): The average number of days a company takes to pay its suppliers.\n\n\\[\\begin{align*}\n    \\text{DIO} &= \\left( \\frac{\\text{Average Inventory}}{\\text{Cost of Goods Sold}} \\right) \\times 365 \\\\\n    \\text{DSO} &= \\left( \\frac{\\text{Average Accounts Receivable}}{\\text{Net Sales}} \\right) \\times 365 \\\\\n    \\text{DPO} &= \\left( \\frac{\\text{Average Accounts Payable}}{\\text{Cost of Goods Sold}} \\right) \\times 365 \\\\\n    \\text{CCC} &= \\text{DIO} + \\text{DSO} - \\text{DPO}\n\\end{align*}\\]\n\n\nDays Inventory Outstanding (DIO):\nAverage Inventory (2023): \\(\\frac{63,843 + 56,481}{2} = 60,162\\)\nCost of Goods Sold (2023): $466,549 million\nDIO = \\(\\left( \\frac{60,162}{466,549} \\right) \\times 365 = 47.07\\) days\nDays Sales Outstanding (DSO)\nAverage Accounts Receivable (2023): \\(\\frac{3,519 + 3,094}{2} = 3,307\\)\nNet Sales (2023): $611,289 million\nDSO = \\(\\left( \\frac{3,307}{611,289} \\right) \\times 365 = 1.97\\) days\nDays Payable Outstanding (DPO)\nAverage Accounts Payable (2023): \\(\\frac{50,257 + 46,092}{2} = 48,175\\)\nCost of Goods Sold (2023): $466,549 million\nDPO = \\(\\left( \\frac{48,175}{466,549} \\right) \\times 365 = 37.69\\) days\nCash-to-Cash Cycle (CCC)\nCCC = \\(47.07 + 1.97 - 37.69 = 11.35\\) days\n\n\n\nDays Inventory Outstanding (DIO)\nAverage Inventory (2023): \\(\\frac{17,907 + 16,651}{2} = 17,279\\)\nCost of Goods Sold (2023): $212,586 million\nDIO = \\(\\left( \\frac{17,279}{212,586} \\right) \\times 365 = 29.70\\) days\nDays Sales Outstanding (DSO)\nAverage Accounts Receivable (2023): \\(\\frac{2,241 + 2,285}{2} = 2,263\\)\nNet Sales (2023): $237,710 million\nDSO = \\(\\left( \\frac{2,263}{237,710} \\right) \\times 365 = 3.47\\) days\nDays Payable Outstanding (DPO)\nAverage Accounts Payable (2023): \\(\\frac{14,838 + 14,903}{2} = 14,871\\)\nCost of Goods Sold (2023): $212,586 million\nDPO = \\(\\left( \\frac{14,871}{212,586} \\right) \\times 365 = 25.52\\) days\nCash-to-Cash Cycle (CCC)\nC2C = \\(29.70 + 3.47 - 25.52 = 7.65\\) days\n\n\n\nWalmart: 11.35 days\nCostco: 7.65 days\n\n\n\nCostco has a shorter cash-to-cash cycle (7.65 days) compared to Walmart (11.35 days), indicating Costco converts its inventory and resources into cash more quickly than Walmart. This efficiency could be attributed to Costco’s business model of rapid inventory turnover and efficient inventory management."
  },
  {
    "objectID": "projects/project4/hw4_questions.html",
    "href": "projects/project4/hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\n\nData\n\n\nCode\nimport pandas as pd\n# Load the dataset\nfile_path = 'data/data_for_drivers_analysis.csv'\ndata = pd.read_csv(file_path)\n# Display the first few rows of the dataset to understand its structure\ndata.head()\n\n\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\n0\n1\n98\n3\n1\n0\n1\n1\n1\n0\n0\n1\n0\n\n\n1\n1\n179\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n197\n3\n1\n0\n0\n1\n1\n1\n0\n1\n1\n\n\n3\n1\n317\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n\n\n4\n1\n356\n4\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\nPearson Correlations\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n#| code-fold: true\n# List of perception columns\nperception_columns = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\n# Compute Pearson correlations\npearson_corr = data[perception_columns].corrwith(data['satisfaction']) * 100\n\npearson_corr_df = pd.DataFrame({'Perception': perception_columns, 'Pearson Correlations': pearson_corr.values})\n\npearson_corr_df\n\n\n\n\n\n\n\n\n\n\nPerception\nPearson Correlations\n\n\n\n\n0\ntrust\n25.570553\n\n\n1\nbuild\n19.189575\n\n\n2\ndiffers\n18.480093\n\n\n3\neasy\n21.298469\n\n\n4\nappealing\n20.799652\n\n\n5\nrewarding\n19.456145\n\n\n6\npopular\n17.142528\n\n\n7\nservice\n25.109823\n\n\n8\nimpact\n25.453855\n\n\n\n\n\n\n\n\n\n\nPolychoric Correlations\n\n\nCode\nfrom scipy.stats import spearmanr\n\n# Calculate Spearman Correlations as an approximation for Polychoric Correlations\nspearman_correlations, _ = spearmanr(data[['satisfaction', 'trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']])\n\n# Extract the correlation values between satisfaction and each perception metric\npolychoric_correlations = spearman_correlations[0, 1:] * 100\n\npolychoric_correlations_df = pd.DataFrame({'Perception': perception_columns, 'Polychoric Correlations': polychoric_correlations})\npolychoric_correlations_df\n\n\n\n\n\n\n\n\n\n\nPerception\nPolychoric Correlations\n\n\n\n\n0\ntrust\n25.261769\n\n\n1\nbuild\n19.505253\n\n\n2\ndiffers\n18.999361\n\n\n3\neasy\n21.218791\n\n\n4\nappealing\n20.439881\n\n\n5\nrewarding\n19.892328\n\n\n6\npopular\n17.135754\n\n\n7\nservice\n25.227913\n\n\n8\nimpact\n26.131387\n\n\n\n\n\n\n\n\nPearson Correlations and Polychoric Correlations indicate the linear and monotonic relationships between each perception and customer satisfaction, respectively. High values suggest stronger relationships.\n\ntrust, service, and impact have the highest correlations, suggesting these factors are strongly associated with customer satisfaction.\npopular has the lowest correlation, indicating it has a weaker relationship with satisfaction.\n\n\n\nStandardized Multiple Regression Coefficients\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize the data\nscaler = StandardScaler()\nX = scaler.fit_transform(data[perception_columns])\ny = scaler.fit_transform(data[['satisfaction']])\n\n# Fit multiple regression model\nreg = LinearRegression().fit(X, y)\n\n# Get standardized coefficients\nstandardized_coefficients = reg.coef_.flatten() * 100\n\nstandardized_coeff_df = pd.DataFrame({'Perception': perception_columns, 'Standardized Multiple Regression Coefficients': standardized_coefficients})\n\nstandardized_coeff_df\n\n\n\n\n\n\n\n\n\n\nPerception\nStandardized Multiple Regression Coefficients\n\n\n\n\n0\ntrust\n11.575180\n\n\n1\nbuild\n1.997929\n\n\n2\ndiffers\n2.784715\n\n\n3\neasy\n2.196988\n\n\n4\nappealing\n3.383472\n\n\n5\nrewarding\n0.506653\n\n\n6\npopular\n1.661571\n\n\n7\nservice\n8.838980\n\n\n8\nimpact\n12.842211\n\n\n\n\n\n\n\n\nStandardized Multiple Regression Coefficients show the direct impact of each perception on customer satisfaction when all other factors are held constant.\n\nimpact and trust have the highest standardized coefficients, suggesting they are the most influential factors.\nrewarding has the lowest coefficient, indicating it has the least direct influence.\n\n\n\nLMG / Shapley Values\n\n\nCode\nfrom sklearn.inspection import permutation_importance\n\n# Fit the Random Forest model\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(data[perception_columns], data['satisfaction'])\n\n# Calculate permutation importance\nperm_importance = permutation_importance(rf, data[perception_columns], data['satisfaction'], n_repeats=30, random_state=42)\n\n# Get the importance values\nshapley_approx = perm_importance.importances_mean * 100\n\nshapley_approx_df = pd.DataFrame({'Perception': perception_columns, 'Approximate Shapley Values': shapley_approx})\n\nshapley_approx_df\n\n\n\n\n\n\n\n\n\n\nPerception\nApproximate Shapley Values\n\n\n\n\n0\ntrust\n17.188167\n\n\n1\nbuild\n12.271351\n\n\n2\ndiffers\n10.549112\n\n\n3\neasy\n12.164332\n\n\n4\nappealing\n11.816158\n\n\n5\nrewarding\n11.881504\n\n\n6\npopular\n11.962427\n\n\n7\nservice\n15.316796\n\n\n8\nimpact\n15.406885\n\n\n\n\n\n\n\n\nShapley Values derived from permutation importance, represent the contribution of each perception to predicting customer satisfaction.\n\ntrust and impact again stand out, contributing significantly to the predictive power of the model.\ndiffers and popular have lower values, indicating they contribute less to the prediction.\n\n\n\nJohnson’s relative weights\n\n\nCode\n# Fit the linear regression model again to ensure we have the correct model\nreg = LinearRegression().fit(data[perception_columns], data['satisfaction'])\n\n# Get the coefficients and the R-squared value\ncoefficients = reg.coef_\nR_squared = reg.score(data[perception_columns], data['satisfaction'])\n\n# Calculate the correlation matrix of the predictors\ncorrelation_matrix = np.corrcoef(data[perception_columns].values.T)\n\n# Calculate the relative weights (Johnson's Epsilon)\nrelative_weights = np.dot(correlation_matrix, coefficients.T) ** 2\nrelative_weights = (relative_weights / np.sum(relative_weights)) * R_squared * 100\n\n# Create a DataFrame for Johnson's Epsilon\njohnsons_epsilon_df = pd.DataFrame({'Perception': perception_columns, \"Johnson's Epsilon\": relative_weights.flatten()})\n\njohnsons_epsilon_df\n\n\n\n\n\n\n\n\n\n\nPerception\nJohnson's Epsilon\n\n\n\n\n0\ntrust\n1.675253\n\n\n1\nbuild\n0.953617\n\n\n2\ndiffers\n0.897269\n\n\n3\neasy\n1.172616\n\n\n4\nappealing\n1.119270\n\n\n5\nrewarding\n0.979661\n\n\n6\npopular\n0.757687\n\n\n7\nservice\n1.621193\n\n\n8\nimpact\n1.724659\n\n\n\n\n\n\n\n\nJohnson’s relative weights Calculated using an algebraic approximation method, these values distribute the R-squared value across the predictors, providing an efficient approximation of their relative importance in a linear regression model.\n\nimpact and trust have the highest Johnson’s Epsilon values, reinforcing their importance in explaining variability in customer satisfaction.\npopular and differs have the lowest values, suggesting they are less critical in the regression model.\n\n\n\nMean Decrease in RF Gini Coefficient\n\n\nCode\n# Define perception columns\nperception_columns = ['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']\n\n# Fit Random Forest model\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(data[perception_columns], data['satisfaction'])\n\n# Get feature importances (Mean Decrease in Gini Coefficient)\nimportances = rf.feature_importances_ * 100\n\n# Create a DataFrame with the importances\nimportance_df = pd.DataFrame({'Perception': perception_columns, 'Mean Decrease in RF Gini Coefficient': importances})\n\nimportance_df\n\n\n\n\n\n\n\n\n\n\nPerception\nMean Decrease in RF Gini Coefficient\n\n\n\n\n0\ntrust\n15.586537\n\n\n1\nbuild\n10.230144\n\n\n2\ndiffers\n8.989693\n\n\n3\neasy\n9.990365\n\n\n4\nappealing\n8.553448\n\n\n5\nrewarding\n10.105692\n\n\n6\npopular\n9.494376\n\n\n7\nservice\n12.966365\n\n\n8\nimpact\n14.083380\n\n\n\n\n\n\n\n\nMean Decrease in RF Gini Coefficient from the Random Forest model indicate how important each feature is in splitting the data to improve homogeneity.\n\ntrust and impact have high importance, aligning with other metrics in showing their strong influence on satisfaction.\ndiffers and appealing have lower importance, indicating they are less crucial for the Random Forest model.\n\n\n\nFinal Table\n\n\nCode\n# Combine all the results into a single DataFrame\nfinal_df = pearson_corr_df.copy()\nfinal_df['Polychoric Correlations'] = polychoric_correlations_df['Polychoric Correlations']\nfinal_df['Standardized Multiple Regression Coefficients'] = standardized_coeff_df['Standardized Multiple Regression Coefficients']\nfinal_df['Approximate Shapley Values'] = shapley_approx_df['Approximate Shapley Values']\nfinal_df[\"Johnson's Epsilon\"] = johnsons_epsilon_df[\"Johnson's Epsilon\"]\nfinal_df['Mean Decrease in RF Gini Coefficient'] = importance_df['Mean Decrease in RF Gini Coefficient']\n\nfinal_df\n\n\n\n\n\n\n\n\n\n\nPerception\nPearson Correlations\nPolychoric Correlations\nStandardized Multiple Regression Coefficients\nApproximate Shapley Values\nJohnson's Epsilon\nMean Decrease in RF Gini Coefficient\n\n\n\n\n0\ntrust\n25.570553\n25.261769\n11.575180\n17.188167\n1.675253\n15.586537\n\n\n1\nbuild\n19.189575\n19.505253\n1.997929\n12.271351\n0.953617\n10.230144\n\n\n2\ndiffers\n18.480093\n18.999361\n2.784715\n10.549112\n0.897269\n8.989693\n\n\n3\neasy\n21.298469\n21.218791\n2.196988\n12.164332\n1.172616\n9.990365\n\n\n4\nappealing\n20.799652\n20.439881\n3.383472\n11.816158\n1.119270\n8.553448\n\n\n5\nrewarding\n19.456145\n19.892328\n0.506653\n11.881504\n0.979661\n10.105692\n\n\n6\npopular\n17.142528\n17.135754\n1.661571\n11.962427\n0.757687\n9.494376\n\n\n7\nservice\n25.109823\n25.227913\n8.838980\n15.316796\n1.621193\n12.966365\n\n\n8\nimpact\n25.453855\n26.131387\n12.842211\n15.406885\n1.724659\n14.083380\n\n\n\n\n\n\n\n\n\nAcross multiple metrics, trust and impact consistently emerge as the most significant factors influencing customer satisfaction. This consistency across different methods (correlation, regression, Shapley values, and random forest importance) strengthens the conclusion that these perceptions are key drivers of satisfaction.\nIn contrast, perceptions like popular and differs are less influential, suggesting that efforts to improve customer satisfaction should prioritize enhancing trust and creating a noticeable impact in customers’ lives."
  },
  {
    "objectID": "projects/456_hw3/456_HW3.html",
    "href": "projects/456_hw3/456_HW3.html",
    "title": "Homework #3",
    "section": "",
    "text": "Product 1\n\nRegional distribution center\n\nimport numpy as np\n\n# Define parameters\nreview_interval = 6  # days\nlead_time = 5  # days\ncycle_service_level = 0.95\nholding_cost_per_unit_per_day = 0.15\ninbound_transportation_cost_per_unit = 0.09\noutbound_transportation_cost_per_unit = 0.10\n\n# Calculate z-score for 95% cycle service level\nz_score = 1.645  # Approximate value for 95% service level\n\n# Extract data for Product 1\nproduct1_data = data['Product1']\n\n# Initialize a dictionary to store the results\nresults = {\n    'Region': [],\n    'OUL': [],\n    'Average Order Quantity': [],\n    'Average Cycle Stock': [],\n    'Average Safety Stock': [],\n    'Average Inventory': [],\n    'Daily Average Inventory Holding Cost': [],\n    'Daily Average Transportation Cost': [],\n    'Total Daily Average Cost': []\n}\n\n# Perform calculations for each region\nfor region in product1_data.columns:\n    daily_demand = product1_data[region]\n    d = daily_demand.mean()\n    sigma_d = daily_demand.std()\n    \n    # Calculate Safety Stock\n    SS = z_score * sigma_d * np.sqrt(lead_time)\n    \n    # Calculate Order-Up-To Level (OUL)\n    OUL = d * (review_interval + lead_time) + SS\n    \n    # Calculate Average Order Quantity\n    average_order_quantity = d * review_interval\n    \n    # Calculate Average Cycle Stock\n    average_cycle_stock = average_order_quantity / 2\n    \n    # Calculate Average Inventory\n    average_inventory = average_cycle_stock + SS\n    \n    # Calculate Daily Average Inventory Holding Cost\n    daily_average_inventory_holding_cost = average_inventory * holding_cost_per_unit_per_day\n    \n    # Calculate Daily Average Transportation Cost\n    daily_average_transportation_cost = (inbound_transportation_cost_per_unit + outbound_transportation_cost_per_unit) * d\n    \n    # Calculate Total Daily Average Cost\n    total_daily_average_cost = daily_average_inventory_holding_cost + daily_average_transportation_cost\n    \n    # Store the results\n    results['Region'].append(region)\n    results['OUL'].append(OUL)\n    results['Average Order Quantity'].append(average_order_quantity)\n    results['Average Cycle Stock'].append(average_cycle_stock)\n    results['Average Safety Stock'].append(SS)\n    results['Average Inventory'].append(average_inventory)\n    results['Daily Average Inventory Holding Cost'].append(daily_average_inventory_holding_cost)\n    results['Daily Average Transportation Cost'].append(daily_average_transportation_cost)\n    results['Total Daily Average Cost'].append(total_daily_average_cost)\n\n# Convert results to DataFrame for better readability\nresults_df = pd.DataFrame(results)\n\nresults_df\n\n\n\n\n\n\n\n\n\nRegion\nOUL\nAverage Order Quantity\nAverage Cycle Stock\nAverage Safety Stock\nAverage Inventory\nDaily Average Inventory Holding Cost\nDaily Average Transportation Cost\nTotal Daily Average Cost\n\n\n\n\n0\nRegion1\n346.640522\n179.252690\n89.626345\n18.010591\n107.636936\n16.145540\n5.676335\n21.821876\n\n\n1\nRegion2\n347.500020\n179.002225\n89.501112\n19.329275\n108.830387\n16.324558\n5.668404\n21.992962\n\n\n2\nRegion3\n349.871140\n180.935009\n90.467504\n18.156957\n108.624461\n16.293669\n5.729609\n22.023278\n\n\n3\nRegion4\n344.533564\n177.996310\n88.998155\n18.206996\n107.205151\n16.080773\n5.636550\n21.717322\n\n\n\n\n\n\n\n\n\n\nNational distribution center\n\n # Define new transportation costs for the National distribution center\nnational_inbound_transportation_cost_per_unit = 0.05\nnational_outbound_transportation_cost_per_unit = 0.24\n\n# Combine the demand data from all regions to represent the National distribution center\nnational_daily_demand = product1_data.sum(axis=1)\nd_national = national_daily_demand.mean()\nsigma_d_national = national_daily_demand.std()\n\n# Calculate Safety Stock for the National distribution center\nSS_national = z_score * sigma_d_national * np.sqrt(lead_time)\n\n# Calculate Order-Up-To Level (OUL) for the National distribution center\nOUL_national = d_national * (review_interval + lead_time) + SS_national\n\n# Calculate Average Order Quantity for the National distribution center\naverage_order_quantity_national = d_national * review_interval\n\n# Calculate Average Cycle Stock for the National distribution center\naverage_cycle_stock_national = average_order_quantity_national / 2\n\n# Calculate Average Inventory for the National distribution center\naverage_inventory_national = average_cycle_stock_national + SS_national\n\n# Calculate Daily Average Inventory Holding Cost for the National distribution center\ndaily_average_inventory_holding_cost_national = average_inventory_national * holding_cost_per_unit_per_day\n\n# Calculate Daily Average Transportation Cost for the National distribution center\ndaily_average_transportation_cost_national = (national_inbound_transportation_cost_per_unit + national_outbound_transportation_cost_per_unit) * d_national\n\n# Calculate Total Daily Average Cost for the National distribution center\ntotal_daily_average_cost_national = daily_average_inventory_holding_cost_national + daily_average_transportation_cost_national\n\n# Store the results in a dictionary\nnational_results = {\n    'Metric': ['OUL', 'Average Order Quantity', 'Average Cycle Stock', 'Average Safety Stock', 'Average Inventory', 'Daily Average Inventory Holding Cost', 'Daily Average Transportation Cost', 'Total Daily Average Cost'],\n    'Value': [OUL_national, average_order_quantity_national, average_cycle_stock_national, SS_national, average_inventory_national, daily_average_inventory_holding_cost_national, daily_average_transportation_cost_national, total_daily_average_cost_national]\n}\n\n# Convert results to DataFrame for better readability\nnational_results_df = pd.DataFrame(national_results)\nnational_results_df\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nOUL\n1353.023838\n\n\n1\nAverage Order Quantity\n717.186233\n\n\n2\nAverage Cycle Stock\n358.593116\n\n\n3\nAverage Safety Stock\n38.182411\n\n\n4\nAverage Inventory\n396.775528\n\n\n5\nDaily Average Inventory Holding Cost\n59.516329\n\n\n6\nDaily Average Transportation Cost\n34.664001\n\n\n7\nTotal Daily Average Cost\n94.180330\n\n\n\n\n\n\n\n\n\nRegional Total Daily Cost: $87.56\nNational Total Daily Cost: $94.18\n\nFor Product 1, the regional distribution centers have a lower total daily cost compared to the national distribution center. Therefore, the regional distribution center model is recommended for Product 1.\n\n\n\nProduct 2\n\nRegional distribution center\n\n\n\n\n\n\n\n\n\n\nRegion\nOUL\nAverage Order Quantity\nAverage Cycle Stock\nAverage Safety Stock\nAverage Inventory\nDaily Average Inventory Holding Cost\nDaily Average Transportation Cost\nTotal Daily Average Cost\n\n\n\n\n0\nRegion1\n124.014636\n61.734266\n30.867133\n10.835148\n41.702281\n6.255342\n1.954918\n8.210261\n\n\n1\nRegion2\n122.857925\n61.005165\n30.502582\n11.015123\n41.517705\n6.227656\n1.931830\n8.159486\n\n\n2\nRegion3\n119.554199\n59.348324\n29.674162\n10.748938\n40.423100\n6.063465\n1.879364\n7.942829\n\n\n3\nRegion4\n124.218983\n62.012635\n31.006318\n10.529152\n41.535470\n6.230320\n1.963733\n8.194054\n\n\n\n\n\n\n\n\n\n\nNational distribution center\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nOUL\n469.171861\n\n\n1\nAverage Order Quantity\n244.100391\n\n\n2\nAverage Cycle Stock\n122.050195\n\n\n3\nAverage Safety Stock\n21.654478\n\n\n4\nAverage Inventory\n143.704674\n\n\n5\nDaily Average Inventory Holding Cost\n21.555701\n\n\n6\nDaily Average Transportation Cost\n11.798186\n\n\n7\nTotal Daily Average Cost\n33.353887\n\n\n\n\n\n\n\n\n\n\n\nProduct 3\n\nRegional distribution center\n\n\n\n\n\n\n\n\n\n\nRegion\nOUL\nAverage Order Quantity\nAverage Cycle Stock\nAverage Safety Stock\nAverage Inventory\nDaily Average Inventory Holding Cost\nDaily Average Transportation Cost\nTotal Daily Average Cost\n\n\n\n\n0\nRegion1\n119.031673\n58.687131\n29.343566\n11.438599\n40.782165\n6.117325\n1.858426\n7.975751\n\n\n1\nRegion2\n117.700347\n58.050420\n29.025210\n11.274577\n40.299787\n6.044968\n1.838263\n7.883231\n\n\n2\nRegion3\n120.443877\n58.874310\n29.437155\n12.507642\n41.944797\n6.291720\n1.864353\n8.156073\n\n\n3\nRegion4\n120.529564\n59.336476\n29.668238\n11.746025\n41.414263\n6.212139\n1.878988\n8.091128\n\n\n\n\n\n\n\n\n\n\nNational distribution center\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nOUL\n468.569137\n\n\n1\nAverage Order Quantity\n234.948337\n\n\n2\nAverage Cycle Stock\n117.474168\n\n\n3\nAverage Safety Stock\n37.830519\n\n\n4\nAverage Inventory\n155.304688\n\n\n5\nDaily Average Inventory Holding Cost\n23.295703\n\n\n6\nDaily Average Transportation Cost\n11.355836\n\n\n7\nTotal Daily Average Cost\n34.651539\n\n\n\n\n\n\n\n\n\nRegional Total Daily Cost(Products 2): $32.51\nNational Total Daily Cost(Products 2): $33.35\nRegional Total Daily Cost(Products 3): $32.11\nNational Total Daily Cost(Products 3): $34.65\n\nFor Products 2 and 3, the regional distribution centers also have a lower total daily cost compared to the national distribution center. Hence, the regional distribution center model is preferable for these products as well.\n\n\n\nWhy National vs. Regional Distribution Centers Differ\nThe differences in costs between national and regional distribution centers can be attributed to several factors:\nTransportation Costs:\n\nNational distribution centers generally have lower inbound transportation costs due to economies of scale but higher outbound transportation costs since the distribution center is located further from the end customers.\nRegional distribution centers, while having higher inbound transportation costs, benefit from lower outbound transportation costs as they are closer to the end customers.\n\nDemand Aggregation:\n\nNational distribution centers can aggregate demand from multiple regions, which can reduce variability and potentially lower safety stock requirements.\nHowever, this aggregation doesn’t always lead to significant savings, especially if the outbound transportation costs are substantially higher.\n\nInventory Holding Costs:\n\nThe holding costs are influenced by the level of safety stock and average inventory. National distribution centers might need higher safety stock due to aggregated demand variability, leading to higher holding costs.\n\nIn this specific case, the regional distribution centers tend to have a cost advantage across all three products, likely due to the balance between transportation costs and inventory holding costs."
  },
  {
    "objectID": "projects/project5/hw5_questions.html",
    "href": "projects/project5/hw5_questions.html",
    "title": "Segmentation Methods",
    "section": "",
    "text": "This analysis demonstrates the implementation and comparison of the k-means clustering algorithm, both custom-built and using the built-in function from the scikit-learn library, on the Iris dataset. The custom k-means algorithm is implemented from scratch, and its performance is visually compared to the results obtained from the optimized scikit-learn KMeans function. The comparison shows that both methods produce similar clusters, validating the correctness of the custom implementation.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport pandas as pd\n\nfile_path = 'data/iris.csv'\niris_data = pd.read_csv(file_path)\n\ndef initialize_centroids(X, k):\n    # Randomly select k unique data points as initial centroids\n    indices = np.random.choice(X.shape[0], k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    # Compute the distance between each data point and the centroids\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    # Assign each data point to the nearest centroid\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    # Compute the mean of the data points in each cluster\n    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    return new_centroids\n\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    centroids = initialize_centroids(X, k)\n    for _ in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        new_centroids = update_centroids(X, labels, k)\n        # Check for convergence\n        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) &lt; tol):\n            break\n        centroids = new_centroids\n    return centroids, labels\n\n# Extract features from the iris dataset\nX = iris_data.iloc[:, :-1].values\n\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Function to plot clusters\ndef plot_clusters(X, labels, centroids, title):\n    plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n    plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='red', marker='X')\n    plt.title(title)\n\n# Apply the custom k-means algorithm\nk = 3\ncustom_centroids, custom_labels = kmeans(X_scaled, k)\n\n# Plot the results of the custom k-means algorithm\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_clusters(X_scaled, custom_labels, custom_centroids, 'Custom k-means')\n\n# Apply the built-in k-means algorithm for comparison\nkmeans_model = KMeans(n_clusters=k, random_state=0)\nkmeans_model.fit(X_scaled)\nbuiltin_centroids = kmeans_model.cluster_centers_\nbuiltin_labels = kmeans_model.labels_\n\n# Plot the results of the built-in k-means algorithm\nplt.subplot(1, 2, 2)\nplot_clusters(X_scaled, builtin_labels, builtin_centroids, 'Built-in k-means')\nplt.show()\n\n\n\n\n\n\n\n\n\nThis code performs the following steps:\n\nData Preprocessing: Extracts and standardizes the features from the Iris dataset.\nCustom k-means Implementation: Uses the custom k-means algorithm on the standardized data.\nVisualization: Plots the clustering results of the custom and built-in k-means algorithms for comparison. ​\n\n\n\n\nCustom k-means:\n\nThe custom k-means algorithm was able to identify clusters and place centroids in positions that attempt to minimize the distance within clusters.\nThe plot on the left shows the results of the custom implementation.\n\nBuilt-in k-means:\n\nThe built-in k-means function from scikit-learn is highly optimized and produces similar clusters and centroids.\nThe plot on the right shows the results of the built-in k-means function.\n\nBoth implementations appear to produce similar clusters, indicating that the custom k-means algorithm is functioning correctly."
  },
  {
    "objectID": "projects/project5/hw5_questions.html#k-means",
    "href": "projects/project5/hw5_questions.html#k-means",
    "title": "Segmentation Methods",
    "section": "",
    "text": "This analysis demonstrates the implementation and comparison of the k-means clustering algorithm, both custom-built and using the built-in function from the scikit-learn library, on the Iris dataset. The custom k-means algorithm is implemented from scratch, and its performance is visually compared to the results obtained from the optimized scikit-learn KMeans function. The comparison shows that both methods produce similar clusters, validating the correctness of the custom implementation.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport pandas as pd\n\nfile_path = 'data/iris.csv'\niris_data = pd.read_csv(file_path)\n\ndef initialize_centroids(X, k):\n    # Randomly select k unique data points as initial centroids\n    indices = np.random.choice(X.shape[0], k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    # Compute the distance between each data point and the centroids\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    # Assign each data point to the nearest centroid\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    # Compute the mean of the data points in each cluster\n    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    return new_centroids\n\ndef kmeans(X, k, max_iters=100, tol=1e-4):\n    centroids = initialize_centroids(X, k)\n    for _ in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        new_centroids = update_centroids(X, labels, k)\n        # Check for convergence\n        if np.all(np.linalg.norm(new_centroids - centroids, axis=1) &lt; tol):\n            break\n        centroids = new_centroids\n    return centroids, labels\n\n# Extract features from the iris dataset\nX = iris_data.iloc[:, :-1].values\n\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Function to plot clusters\ndef plot_clusters(X, labels, centroids, title):\n    plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n    plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='red', marker='X')\n    plt.title(title)\n\n# Apply the custom k-means algorithm\nk = 3\ncustom_centroids, custom_labels = kmeans(X_scaled, k)\n\n# Plot the results of the custom k-means algorithm\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplot_clusters(X_scaled, custom_labels, custom_centroids, 'Custom k-means')\n\n# Apply the built-in k-means algorithm for comparison\nkmeans_model = KMeans(n_clusters=k, random_state=0)\nkmeans_model.fit(X_scaled)\nbuiltin_centroids = kmeans_model.cluster_centers_\nbuiltin_labels = kmeans_model.labels_\n\n# Plot the results of the built-in k-means algorithm\nplt.subplot(1, 2, 2)\nplot_clusters(X_scaled, builtin_labels, builtin_centroids, 'Built-in k-means')\nplt.show()\n\n\n\n\n\n\n\n\n\nThis code performs the following steps:\n\nData Preprocessing: Extracts and standardizes the features from the Iris dataset.\nCustom k-means Implementation: Uses the custom k-means algorithm on the standardized data.\nVisualization: Plots the clustering results of the custom and built-in k-means algorithms for comparison. ​\n\n\n\n\nCustom k-means:\n\nThe custom k-means algorithm was able to identify clusters and place centroids in positions that attempt to minimize the distance within clusters.\nThe plot on the left shows the results of the custom implementation.\n\nBuilt-in k-means:\n\nThe built-in k-means function from scikit-learn is highly optimized and produces similar clusters and centroids.\nThe plot on the right shows the results of the built-in k-means function.\n\nBoth implementations appear to produce similar clusters, indicating that the custom k-means algorithm is functioning correctly."
  },
  {
    "objectID": "projects/456_hw3/HW3_sol.html",
    "href": "projects/456_hw3/HW3_sol.html",
    "title": "product 1",
    "section": "",
    "text": "#——————# # ALCO case write up: Answer code #——————# #——————# #————————-# # Load data #————————-# import math import pandas as pd import numpy as np\n#computes the mean of list of values def mean(listOfValues): listOfValuesTmp = [x for x in listOfValues if x] return sum(listOfValuesTmp)/len(listOfValuesTmp)\ncv = lambda x: np.std(x, ddof=1) / np.mean(x)\nxls = pd.ExcelFile(‘Homework3_data.xlsx’) product1 = pd.read_excel(xls, ‘Product1’) product2 = pd.read_excel(xls, ‘Product2’) product3 = pd.read_excel(xls, ‘Product3’)\nprint(product1.describe()) print(product2.describe()) print(product3.describe())\n#————————-# # Settings #————————-# lt = 11 tSL = 0.95 #————————-# # Analysis #————————-# def calculateInventoryCost(tSL, shippingCost, dfSite): rollingSum = dfSite.rolling(11, closed=‘right’, ).sum() ltDemandTmp = list(pd.Series([np.NaN]).append(rollingSum))[1:] ltDemand = [x for x in ltDemandTmp if not math.isnan(x)] OUL = np.quantile(ltDemand, tSL) SS = OUL - lt * mean(dfSite) OrderQ = 6 * mean(dfSite) cycleStock = OrderQ / 2 inventory = SS + cycleStock holdingCost = inventory * 0.15 ShippingCost = mean(dfSite) * shippingCost return { ‘CycleStock’ : cycleStock,\n‘OUL’ : OUL, ‘SafetyStock’ : SS, ‘Inventory’ : inventory, ‘HoldingCost’ : holdingCost, ‘ShippingCost’ : ShippingCost, “OrderQ”:OrderQ}\n\nproduct 1\nRegion1 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product1[‘Region1’]) Region2 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product1[‘Region2’]) Region3 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product1[‘Region3’]) Region4 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product1[‘Region4’]) Central = calculateInventoryCost(tSL = tSL, shippingCost = 0.29, dfSite = product1.sum(axis=1))\nprint(“Product 1 holding cost :”,Region1[‘HoldingCost’] + Region2[‘HoldingCost’] + Region3[‘HoldingCost’] + Region4[‘HoldingCost’]) print(“Product 1 shipping cost :”,Region1[‘ShippingCost’] + Region2[‘ShippingCost’] + Region3[‘ShippingCost’] + Region4[‘ShippingCost’])\nprint(“Product 1 Central holding cost”,Central[‘HoldingCost’]) print(“Product 1 Central shipping cost”,Central[‘ShippingCost’])\nprint(product1.apply(cv)) print(product1.corr()) print(“—————————————————————”) # product 2 Region1 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product2[‘Region1’]) Region2 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product2[‘Region2’]) Region3 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product2[‘Region3’]) Region4 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product2[‘Region4’]) Central = calculateInventoryCost(tSL = tSL, shippingCost = 0.29, dfSite = product2.sum(axis=1))\nprint(“Product 2 holding cost :”,Region1[‘HoldingCost’] + Region2[‘HoldingCost’] + Region3[‘HoldingCost’] + Region4[‘HoldingCost’]) print(“Product 2 shipping cost :”,Region1[‘ShippingCost’] + Region2[‘ShippingCost’] + Region3[‘ShippingCost’] + Region4[‘ShippingCost’])\nprint(“Product 2 Central holding cost”,Central[‘HoldingCost’]) print(“Product 2 Central shipping cost”,Central[‘ShippingCost’])\nprint(product2.apply(cv)) print(product2.corr()) print(“—————————————————————”) # product 3 Region1 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product3[‘Region1’]) Region2 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product3[‘Region2’]) Region3 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product3[‘Region3’]) Region4 = calculateInventoryCost(tSL = tSL, shippingCost = 0.19, dfSite = product3[‘Region4’]) Central = calculateInventoryCost(tSL = tSL, shippingCost = 0.29, dfSite = product3.sum(axis=1))\nprint(“Product 3 holding cost :”,Region1[‘HoldingCost’] + Region2[‘HoldingCost’] + Region3[‘HoldingCost’] + Region4[‘HoldingCost’]) print(“Product 3 shipping cost :”,Region1[‘ShippingCost’] + Region2[‘ShippingCost’] + Region3[‘ShippingCost’] + Region4[‘ShippingCost’]) print(“Product 3 holding cost :”,Central[‘HoldingCost’]) print(“Product 3 shipping cost :”,Central[‘ShippingCost’])\nprint(product3.apply(cv)) print(product3.corr()) print(“—————————————————————”)"
  }
]